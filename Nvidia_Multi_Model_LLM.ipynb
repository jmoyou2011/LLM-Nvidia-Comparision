{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmoyou2011/LLM-Nvidia-Comparision/blob/main/Nvidia_Multi_Model_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JTQI4ZgTxHZ3"
      },
      "outputs": [],
      "source": [
        "import requests as req\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "import time\n",
        "import yaml\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a mount point for data"
      ],
      "metadata": {
        "id": "6L4yXruH764O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/MyDrive/ColabNotebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSWXMjT17rbV",
        "outputId": "53e0c906-7c1f-4555-f1a3-270be3755ca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization\n",
        "These will be the global variables used by the proceeding LLM functions.\n",
        "\n",
        "Note: I renamed the file \"Colab Notebooks\" to \"ColabNotebooks\" to facilitate\n",
        "use of !cat feature to produce results.\n"
      ],
      "metadata": {
        "id": "uVFc17DP8CVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/ColabNotebooks/model_names.yml') as f:\n",
        "  config = yaml.safe_load(f)\n",
        "\n",
        "my_api = config['KEYS'][0]\n",
        "model_dict = config['model_names']\n",
        "model_keys = list(model_dict.keys())"
      ],
      "metadata": {
        "id": "OTMCxBoY7vXQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nvidia LLM\n",
        "\n",
        "All the LLM are extracted from the Nvidia LLM foundational AI models catalog page. Using the APIs provided on the doc page, we will comparing them between\n",
        "each other an displaying the results through gradio module."
      ],
      "metadata": {
        "id": "B77aA7z48ijE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_invoke(model_name:str, prompt:str):\n",
        "  \"\"\"\n",
        "    This function will call from any model within the dictionary from Nvidia\n",
        "    AI foundational models and run the model. This is strictly focused on\n",
        "    the text to text models found on the link below:\n",
        "\n",
        "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
        "\n",
        "    Models that required context are not included in the dictionary.\n",
        "\n",
        "    Inputs\n",
        "    model_name -> name of the model\n",
        "    prompt -> prompt to be passed to the model\n",
        "\n",
        "    Outputs\n",
        "    msg -> text generated from the model given the prompt\n",
        "    resp_time -> time taken to generate the response\n",
        "    out_Tokens -> Number of tokens returned from the LLM.\n",
        "\n",
        "  \"\"\"\n",
        "  model_name = model_name.lower().replace(\" \", \"\")\n",
        "\n",
        "  invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/\"\n",
        "  fetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\n",
        "\n",
        "  headers = {\n",
        "    \"Authorization\": \"Bearer \" + str(my_api),\n",
        "    \"Accept\": \"application/json\",\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"content\": str(prompt),\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ],\n",
        "  \"temperature\": 0.2,\n",
        "  \"top_p\": 0.7,\n",
        "  \"max_tokens\": 1024,\n",
        "  \"seed\": 42,\n",
        "  \"stream\": False\n",
        "  }\n",
        "\n",
        "  if model_name not in model_dict.keys():\n",
        "    print(\"Model name not found in dictionary, using default model\")\n",
        "    print(\"Default model is NV-Llama2-70B-RLHF\")\n",
        "    model_name = \"nv-llama2-70b-rlhf\"\n",
        "\n",
        "  #Create session.\n",
        "  session = req.Session()\n",
        "\n",
        "  response = session.post(invoke_url + model_dict[model_name], headers=headers, json=payload)\n",
        "\n",
        "  while response.status_code == 202:\n",
        "    request_id = response.headers.get(\"NVCF-REQID\")\n",
        "    fetch_url = fetch_url_format + request_id\n",
        "    response = session.get(fetch_url, headers=headers)\n",
        "\n",
        "  response.raise_for_status()\n",
        "  response_body = response.json()\n",
        "  msg = response_body.get('choices')[0].get('message').get('content')\n",
        "  resp_time = round(response.elapsed.total_seconds(), 3)\n",
        "  out_tokens = response_body.get('usage').get('completion_tokens')\n",
        "  return msg, resp_time, out_tokens\n",
        "\n",
        "\n",
        "\n",
        "def llm_invoke_all(prompt:str):\n",
        "  \"\"\"\n",
        "    This function will call from any model within the dictionary from Nvidia\n",
        "    AI foundational models and run the model. This is strictly focused on\n",
        "    the text to text models found on the link below:\n",
        "\n",
        "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
        "\n",
        "    Models that required context are not included in the dictionary.\n",
        "\n",
        "    Inputs\n",
        "    prompt -> Prompt to be passed to the model\n",
        "\n",
        "    Outputs\n",
        "    msg -> Text generated from the model given the prompt\n",
        "    Resp_time -> Time taken to generate the response\n",
        "    out_tokens -> Number of tokens produced by the LLM\n",
        "    model_name -> Name of the model that was called by the curl request.\n",
        "\n",
        "  \"\"\"\n",
        "  lst_resp = []\n",
        "\n",
        "  invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/\"\n",
        "  fetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\n",
        "\n",
        "  headers = {\n",
        "    \"Authorization\": \"Bearer \" + str(my_api),\n",
        "    \"Accept\": \"application/json\",\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"content\": str(prompt),\n",
        "      \"role\": \"user\"\n",
        "    }\n",
        "  ],\n",
        "  \"temperature\": 0.2,\n",
        "  \"top_p\": 0.7,\n",
        "  \"max_tokens\": 1024,\n",
        "  \"seed\": 42,\n",
        "  \"stream\": False\n",
        "  }\n",
        "\n",
        "  #Create session.\n",
        "  session = req.Session()\n",
        "\n",
        "  for key, _ in model_dict.items():\n",
        "    # print(key)\n",
        "    tmp_dict = {}\n",
        "    response = session.post(invoke_url + model_dict[key], headers=headers, json=payload)\n",
        "\n",
        "    while response.status_code == 202:\n",
        "      request_id = response.headers.get(\"NVCF-REQID\")\n",
        "      fetch_url = fetch_url_format + request_id\n",
        "      response = session.get(fetch_url, headers=headers)\n",
        "\n",
        "    response.raise_for_status()\n",
        "    response_body = response.json()\n",
        "    msg = response_body.get('choices')[0].get('message').get('content')\n",
        "    resp_time = round(response.elapsed.total_seconds(), 3)\n",
        "    out_tokens = response_body.get('usage').get('completion_tokens')\n",
        "    tmp_dict = {\"msg\": msg, \"resp_time\": resp_time, \"model_name\": key, \"output_tokens\": out_tokens}\n",
        "    lst_resp.append(tmp_dict)\n",
        "    time.sleep(2)\n",
        "\n",
        "  lst_resp = sorted(lst_resp, key=lambda x: x['resp_time'], reverse = False)\n",
        "  return lst_resp\n"
      ],
      "metadata": {
        "id": "OLyZlVqf9d_C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the first function.\n",
        "msg, resp_time, out_tokens = llm_invoke(\"nv-llama2-70b-rlhf\", \"Describe the stonehedge in the british isle? Your persona is a druid.\")\n",
        "\n",
        "print(msg)\n",
        "print(\"Response time:\", resp_time, \" seconds\")\n",
        "print(\"Tokens Produced:\", out_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwdzHhBEvjzn",
        "outputId": "dd948281-6365-479f-9aab-969a2178ec93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Druid, I can tell you that Stonehenge is a sacred site that holds great significance for our people. It is a prehistoric monument located in the British Isle of Wight, and is believed to have been built by our ancestors over 5,000 years ago. The stones that make up Stonehenge are massive, standing up to 24 feet tall and weighing up to 50 tons each. It is a testament to the ingenuity and skill of our ancestors that they were able to transport and erect these stones without the use of modern tools and technology.\n",
            "\n",
            "Stonehenge is a place of great mystical power, and is believed to have been used for a variety of purposes, including rituals, healing, and astronomical observations. The alignment of the stones with the sun and the moon is particularly noteworthy, as it suggests that our ancestors had a deep understanding of the movements of the celestial bodies.\n",
            "\n",
            "For Druids, Stonehenge is a place of worship and a connection to the ancient past. We believe that the stones have a powerful energy that can be harnessed for healing and spiritual growth. It is a place where we can come to connect with the natural world and the spirits of our ancestors.\n",
            "\n",
            "Today, Stonehenge remains a popular tourist attraction, drawing visitors from all over the world who come to marvel at its grandeur and mystery. But for Druids, it is so much more than just a historical site. It is a sacred place that holds the secrets of our past and the key to our spiritual future.\n",
            "Response time: 1.407  seconds\n",
            "Tokens Produced: 349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the second function.\n",
        "llm_responses = llm_invoke_all(\"Describe the catacombs in Paris?\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/ColabNotebooks/response_llm.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(llm_responses, f, indent=6, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "hmh5N-t5-I3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Functionality"
      ],
      "metadata": {
        "id": "PFkGs8DH--Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja1dUiNa-hjg",
        "outputId": "eb2b0d24-e5c7-4298-9aca-2dc9dba01234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.17.0-py3-none-any.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.9.0 (from gradio)\n",
            "  Downloading gradio_client-0.9.0-py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.8/306.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.8-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.1.7 (from gradio)\n",
            "  Downloading ruff-0.2.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.9.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.9.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=4175ae26119cd6c8a6b741a67be7c1adbffdc9cc6bc886bb30f5be59685e3dd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.109.2 ffmpy-0.3.1 gradio-4.17.0 gradio-client-0.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 orjson-3.9.13 pydub-0.25.1 python-multipart-0.0.8 ruff-0.2.1 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.27.0.post1 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio Interfaces.\n",
        "There will be three gradio interfaces that a user can interact with in this\n",
        "notebook\n",
        "\n",
        " * Choose a model and enter a prompt to return a response from the llm\n",
        " * Evaluation of the most responsive model over the eleven models being evaluated\n",
        " * File upload to view the full results of the function llm_invoke_all."
      ],
      "metadata": {
        "id": "atk7T4B3_WvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Formatting\n",
        "def llm_response_gradio(model_name: str, prompt: str):\n",
        "    msg, resp_time = llm_invoke(model_name, prompt)\n",
        "    output = f\"{msg}\\n\\nResponse time: {resp_time} seconds\"\n",
        "    return output\n",
        "\n",
        "def llm_response_gradio_all(prompt: str):\n",
        "    content_lst = list()\n",
        "    response_lst = llm_invoke_all(prompt)\n",
        "    for doc in response_lst:\n",
        "        content = f\"Model:{doc.get('model_name')}\\n\\nResponse Time:{doc.get('resp_time')}\\n\\nTokens Produced:{doc.get('output_tokens')}\\n\\n\"\n",
        "        content_lst.append(content)\n",
        "    return \" \".join(content_lst)\n",
        "\n",
        "def read_file(file):\n",
        "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "    return f.read()\n"
      ],
      "metadata": {
        "id": "Too_UnbZ-79o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface_singular = gr.Interface(fn = llm_response_gradio,\n",
        "                     inputs =[gr.Radio(list(model_dict.keys()), label=\"Model Name\"),\n",
        "                              gr.Textbox(label=\"Enter your Prompt\")],\n",
        "                     outputs = gr.Textbox(label=\"LLM Output\"),\n",
        "                     title = \"Nvidia LLM Invoker\",\n",
        "                     description = \"Choose a model and enter a prompt to invoke a LLM model from Nvidia AI Foundational Models.\")\n",
        "iface_singular.launch()"
      ],
      "metadata": {
        "id": "B07gAa9eATKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface_singular.close()"
      ],
      "metadata": {
        "id": "q3BF_BBdAaKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface_llm = gr.Interface(fn = llm_response_gradio_all,\n",
        "                         inputs = gr.Textbox(label=\"Enter your Prompt\"),\n",
        "                         outputs = gr.Textbox(label=\"Best Performing LLMs with respect to time.\"),\n",
        "                         title = \"Nvidia Multi-Model Invoker\",\n",
        "                         description = \"Enter a prompt to invoke multiple LLM model from Nvidia AI Foundational Models and return the fastest 3.\"\n",
        ")\n",
        "\n",
        "iface_llm.launch()"
      ],
      "metadata": {
        "id": "jGKibJhWAdoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface_llm.close()"
      ],
      "metadata": {
        "id": "iI9Wlu9kAdte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface_read = gr.Interface(fn=read_file, inputs=\"file\", outputs=\"text\")\n",
        "iface_read.launch()"
      ],
      "metadata": {
        "id": "jZ5ASZ8XAd0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface_read.close()"
      ],
      "metadata": {
        "id": "TMqlFBq_Ad-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
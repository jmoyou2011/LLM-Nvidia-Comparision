{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6005e838-aa25-4e97-9bc3-fc66fbb19a31",
   "metadata": {},
   "source": [
    "# Nvidia Foundational Large Language Models\n",
    "\n",
    "The Nvidia **foundational Large Language Models** (LLMs) are hosted on the catalog.ngc.nvidia.com webpage.\n",
    "Nvidia provides these models through either streaming or non-streaming process APIs for anyone\n",
    "to interact with many models. Nvidia provides an API key to developers to interact with these \n",
    "LLMs without a cost such as the OPENAI API and the rate-limiting of OPENAI. This allows new developers to\n",
    "experiment with LLMs with limited knowledge of the underlying architecture to obtain responses from \n",
    "prompts supplied.\n",
    "\n",
    "In this notebook, we will look at 11 models that are labeled text-to-text models on their catalog. Since\n",
    "all the APIs for the models are the same except the last path item which is the uuid for the model. These APIs\n",
    "will be called using the Python requests module. Other request modules such as async with aiohttp, \n",
    "and httpx.\n",
    "\n",
    "#### NOTE\n",
    "There was experimentation with aiohttp with asyncio with the API calls. There are a few issues discovered:\n",
    "\n",
    "    1. Some models will return a 200 status code instead of the expected 202. This required a rewrite to account\n",
    "       for this problem.\n",
    "       \n",
    "    2. This problem is variable in nature as it is not consistent to a particular model. If you want to async\n",
    "       call the API for each model for a single prompt, one to a few of the models will return with an empty\n",
    "       response. The amount of models that return an empty response will vary with each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50e56ff7-f7c7-4ab0-b0bb-c4b9b3745e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import os, sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd \n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a271c-3d3e-4567-92c6-0e664329cb01",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "In the cell below we will initialize our global variables that will be used by the LLM functions.\n",
    "\n",
    "#### NOTE:\n",
    "At the time of making this notebook, Mamba-chat had been released to the Nvidia catalog page on 02/12/2024.\n",
    "It was not included in the set of testing due to the payload differing in structure from the other which will\n",
    "require a rework to include.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d20114-ba03-4923-a7ab-63ef4cae03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"mixtral8x7binstruct\": \"8f4118ba-60a8-4e6b-8574-e38a4067a4a3\",\n",
    "    \"mistral7binstruct\": \"35ec3354-2681-4d0e-a8dd-80325dcf7c63\",\n",
    "    \"nv-llama2-70b-rlhf\": \"7b3e3361-4266-41c8-b312-f5e33c81fc92\",\n",
    "    \"nv-llama2-70b-steerlm\": \"d6fe6881-973a-4279-a0f8-e1d486c9618d\",\n",
    "    \"codellama13b\": \"f6a96af4-8bf9-4294-96d6-d71aa787612e\",\n",
    "    \"codellama70b\": \"2ae529dc-f728-4a46-9b8d-2697213666d8\",\n",
    "    \"codellama34b\": \"df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8\",\n",
    "    \"llama213b\": \"e0bb7fb9-5333-4a27-8534-c6288f921d3f\",\n",
    "    \"llama270b\": \"0e349b44-440a-44e1-93e9-abe8dcb27158\",\n",
    "    \"yi-34b\": \"347fa3f3-d675-432c-b844-669ef8ee53df\",\n",
    "    \"nemotron-3-8b-chat-steerlm\": \"1423ff2f-d1c7-4061-82a7-9e8c67afd43a\",\n",
    "}\n",
    "\n",
    "invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/\"\n",
    "fetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\n",
    "\n",
    "model_urls = {}\n",
    "for k, v in model_dict.items():\n",
    "    model_urls[k] = invoke_url + v\n",
    "\n",
    "# This method is not the safest way to initialize an API key. This is meant for demonstration.\n",
    "# Insert your ngc catalog API key which can be found on the model's catalog page.\n",
    "os.environ[\"NGCKEY\"] = \"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\"\n",
    "my_api = os.environ.get('NGCKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f2d6f-3f23-4ce4-85fe-cc30e85a3cd1",
   "metadata": {},
   "source": [
    "### LLM Functions\n",
    "Two main LLM functions interact with the Nvidia LLMs. All the models\n",
    "are interfacing with Nvidia Triton. Some of the models indicate what GPU/GPUS are\n",
    "used for the interfacing while some indicate others. Comparisons may not be \"apple-to-apple\"\n",
    "due to not all models interfacing with the same hardware.\n",
    "\n",
    "One function named \"llm_invoke\" works by invoking a model name and prompt to return the model\n",
    "response. This works with a helper function that formats the response to be displayed in the gradio\n",
    "interface.\n",
    "\n",
    "The second function is similar to the first model saves the output in the list of dictionaries with a\n",
    "similar gradio formatting function supplementing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a986ec47-cb9d-401c-b985-d32487a82b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_invoke(model_name:str, prompt:str):\n",
    "  \"\"\"\n",
    "    This function will call from any model within the dictionary from Nvidia\n",
    "    AI foundational models and run the model. This is strictly focused on\n",
    "    the text-to-text models found on the link below:\n",
    "\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
    "\n",
    "    Models that require context are not included in the dictionary.\n",
    "\n",
    "    Inputs\n",
    "    model_name (str): name of the model\n",
    "    prompt (str): prompt to be passed to the model\n",
    "\n",
    "    Outputs\n",
    "    msg (str): text generated from the model given the prompt\n",
    "    resp_time (str): time taken to generate the response\n",
    "    out_Tokens (str): Number of tokens returned from the LLM.\n",
    "\n",
    "  \"\"\"\n",
    "  model_name = model_name.lower().replace(\" \", \"\")\n",
    "\n",
    "  headers = {\n",
    "    \"Authorization\": \"Bearer \" + str(my_api),\n",
    "    \"Accept\": \"application/json\",\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": str(prompt),\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"seed\": 42,\n",
    "  \"stream\": False\n",
    "  }\n",
    "\n",
    "  if model_name not in model_dict.keys():\n",
    "    print(\"Model name not found in dictionary, using default model\")\n",
    "    print(\"Default model is NV-Llama2-70B-RLHF\")\n",
    "    model_name = \"nv-llama2-70b-rlhf\"\n",
    "\n",
    "  #Create session.\n",
    "  session = req.Session()\n",
    "\n",
    "  response = session.post(model_urls[model_name], headers=headers, json=payload)\n",
    "\n",
    "  while response.status_code == 202:\n",
    "    request_id = response.headers.get(\"NVCF-REQID\")\n",
    "    fetch_url = fetch_url_format + request_id\n",
    "    response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "  response.raise_for_status()\n",
    "  response_body = response.json()\n",
    "  msg = response_body.get('choices')[0].get('message').get('content')\n",
    "  resp_time = round(response.elapsed.total_seconds(), 3)\n",
    "  out_tokens = response_body.get('usage').get('completion_tokens')\n",
    "  return msg, resp_time, out_tokens\n",
    "\n",
    "def llm_response_gradio(model_name: str, prompt: str):\n",
    "    msg, resp_time, out_tokens = llm_invoke(model_name, prompt)\n",
    "    output = f\"{msg}\\n\\nResponse time: {resp_time} seconds\\n\\nOutput_tokens:{out_tokens}\"\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba04eb7-c707-4e57-bab4-ade99f0c635e",
   "metadata": {},
   "source": [
    "#### Testing the llm invocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ec7841-25e3-45ec-8d7a-3fea5b264907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Result:\n",
      "Paris is a city with a rich history, art, architecture, and culture. Here are five of the best spots to visit in Paris:\n",
      "1. The Eiffel Tower: This iconic tower is one of the most recognizable landmarks in the world. Visitors can take the elevator to the top of the tower for panoramic views of the city.\n",
      "2. The Louvre Museum: This world-renowned museum is home to some of the most famous and iconic works of art in the world, including the Mona Lisa. Visitors can take a guided tour of the museum to learn more about the history and significance of the works of art on display.\n",
      "3. The Arc de Triomphe: This iconic monument is one of the most recognizable landmarks in the world. Visitors can take a guided tour of the monument to learn more about its history and significance.\n",
      "4. The Notre Dame Cathedral: This world-renowned cathedral is one of the most recognizable landmarks in the world. Visitors can take a guided tour of the cathedral to learn more about its history and significance.\n",
      "5. The Champs Elysees: This iconic street is one of the most recognizable landmarks in the world. Visitors can take a guided tour of the street to learn more about its history and significance.\n",
      "\n",
      "Response time: 0.799 seconds\n",
      "\n",
      "Output_tokens:1023\n"
     ]
    }
   ],
   "source": [
    "test_prompt_singular = \"I am visiting paris, what should I see? Limit to five best spots.\"\n",
    "result_singular = llm_response_gradio(\"codellama13b\", test_prompt_singular)\n",
    "print(\"LLM Result:\\n{}\".format(result_singular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47b7a15-7634-48b2-ab99-2e79496741c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_invoke_all(prompt:str):\n",
    "  \"\"\"\n",
    "    This function will call from any model within the dictionary from Nvidia\n",
    "    AI foundational models and run the model. This is strictly focused on\n",
    "    the text to text models found on the link below:\n",
    "\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
    "\n",
    "    Models that required context are not included in the dictionary.\n",
    "\n",
    "    Inputs\n",
    "    prompt -> Prompt to be passed to the model\n",
    "\n",
    "    Outputs\n",
    "    msg -> Text generated from the model given the prompt\n",
    "    Resp_time -> Time taken to generate the response\n",
    "    out_tokens -> Number of tokens produced by the LLM\n",
    "    model_name -> Name of the model that was called by the curl request.\n",
    "\n",
    "  \"\"\"\n",
    "  lst_resp = []\n",
    "  headers = {\n",
    "    \"Authorization\": \"Bearer \" + str(my_api),\n",
    "    \"Accept\": \"application/json\",\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": str(prompt),\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"seed\": 42,\n",
    "  \"stream\": False\n",
    "  }\n",
    "\n",
    "  #Create session.\n",
    "  session = req.Session()\n",
    "\n",
    "  for key, url in model_urls.items():\n",
    "    tmp_dict = {}\n",
    "    response = session.post(url, headers = headers, json = payload)\n",
    "\n",
    "    while response.status_code == 202:\n",
    "      request_id = response.headers.get(\"NVCF-REQID\")\n",
    "      fetch_url = fetch_url_format + request_id\n",
    "      response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "    response_body = response.json()\n",
    "    msg = response_body.get('choices')[0].get('message').get('content')\n",
    "    resp_time = round(response.elapsed.total_seconds(), 3)\n",
    "    out_tokens = response_body.get('usage').get('completion_tokens')\n",
    "    tmp_dict = {\"model_name\": key,\"resp_time\": resp_time, \"output_tokens\": out_tokens,\n",
    "                \"prompt\": prompt, \"resp_msg\": msg}\n",
    "    lst_resp.append(tmp_dict)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "  return lst_resp\n",
    "\n",
    "def llm_response_gradio_all(prompt: str):\n",
    "    content_lst = list()\n",
    "    response_lst = llm_invoke_all(prompt)\n",
    "    response_lst = sorted(response_lst, key=lambda x: x['resp_time'], reverse = False)\n",
    "    for doc in response_lst:\n",
    "        content = f\"Model:{doc.get('model_name')}\\n\\nResponse Time:{doc.get('resp_time')}\\n\\nTokens Produced:{doc.get('output_tokens')}\\n\\n\"\n",
    "        content_lst.append(content)\n",
    "    return \" \".join(content_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e5fe7-44cf-4df2-b377-5c0bc2b89385",
   "metadata": {},
   "source": [
    "### Creating benchmark datasets\n",
    "\n",
    "The section below creates the LLMs results dataframes and LLM benchmark resutls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71fe3822-b3f4-4713-aee7-07e9f8814d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_dataset(file_path:str) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Creates a data frame that is the product of running multiple\n",
    "    prompts through all the models and returns the response\n",
    "    message, model response time, and output tokens.\n",
    "\n",
    "    Inputs:\n",
    "    file_path (str): csv file path containing a singular column of prompts\n",
    "\n",
    "    Outputs:\n",
    "    dataset (pd.DataFrame): Output data frame of all prompts apply\n",
    "                            to the function of calling all LLM models.\n",
    "\n",
    "    \"\"\"\n",
    "    df_prompts = pd.read_csv(file_path, sep=\",\")\n",
    "    lst_prompts = df_prompts['questions'].values\n",
    "\n",
    "    lst_responses = []\n",
    "    counter = 0\n",
    "    print(\"Starting LLM API calls\")\n",
    "    for prompts in lst_prompts:\n",
    "        full_llm_res = llm_invoke_all(prompts)\n",
    "        lst_responses.append(full_llm_res)\n",
    "        counter += 1\n",
    "        print(\"Completed {} rounds\".format(counter))\n",
    "\n",
    "    lst_pd = []\n",
    "    for sublist in lst_responses:\n",
    "        for item in sublist:\n",
    "            lst_pd.append(item)\n",
    "\n",
    "    df_results = pd.DataFrame(lst_pd)\n",
    "    return df_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c2d7a6-d106-462d-8930-17fd2035dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_llm(llm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a data frame that is a merge of two groupby \n",
    "    data frames of response time and output token.\n",
    "\n",
    "    Inputs: \n",
    "    llm_df (pd.DataFrame): Data frame of LLM responses\n",
    "\n",
    "    Outputs:\n",
    "    stats_df (pd.DataFrame): Data frame of LLM aggregation stats.\n",
    "    \n",
    "    \"\"\"\n",
    "    time_df = llm_df.groupby(\"model_name\")['resp_time'].aggregate(resp_time_min='min',\n",
    "                                                                  resp_time_max='max',resp_time_mean='mean')\n",
    "    time_df = time_df.reset_index()\n",
    "    time_df = time_df.sort_values('resp_time_mean', ascending=True).reset_index(0, drop=True)\n",
    "\n",
    "    tkn_df = llm_df.groupby(\"model_name\")['output_tokens'].aggregate(out_tkn_min=\"min\", out_tkn_max=\"max\", out_tkn_mean=\"mean\")\n",
    "    tkn_df = tkn_df.reset_index()\n",
    "    tkn_df = tkn_df.sort_values(\"out_tkn_mean\", ascending=True).reset_index(0, drop=True)\n",
    "\n",
    "    stats_df = time_df.merge(tkn_df, how=\"inner\", on=\"model_name\")\n",
    "\n",
    "    return stats_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a56b7503-340c-42e9-8405-456832890a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_save(llm_df: pd.DataFrame, stats_llm: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Saves two data frames to separate json files.   \n",
    "    \"\"\"\n",
    "    folder_path = \"./output_llm/\"\n",
    "    os.makedirs(folder_path, exist_ok = True)\n",
    "\n",
    "    llm_df.to_json(folder_path + \"results_llm.json\", orient = \"records\", compression = \"infer\")\n",
    "\n",
    "    stats_llm.to_json(folder_path + \"stats_llm.json\", orient = \"records\", compression = \"infer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c5ac11d-6de5-4695-9766-3ade89884ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM API calls\n",
      "Completed 1 rounds\n",
      "Completed 2 rounds\n",
      "Completed 3 rounds\n",
      "Completed 4 rounds\n",
      "Completed 5 rounds\n",
      "Completed 6 rounds\n",
      "Completed 7 rounds\n",
      "Completed 8 rounds\n",
      "Completed 9 rounds\n",
      "Completed 10 rounds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'elasped_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m end_tm \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m (end_tm \u001b[38;5;241m-\u001b[39m start_tm) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time to run benchmark: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(\u001b[43melasped_time\u001b[49m,\u001b[38;5;241m3\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elasped_time' is not defined"
     ]
    }
   ],
   "source": [
    "csv_path = \"./data/questions.csv\"\n",
    "start_tm = time.time()\n",
    "df_llm = create_llm_dataset(csv_path)\n",
    "llm_stats = create_stats_llm(df_llm)\n",
    "end_tm = time.time()\n",
    "elapsed_time = (end_tm - start_tm) / 60\n",
    "print(\"Elapsed time to run benchmark: {} seconds\".format(round(elapsed_time,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e42ed379-84c2-4290-b6ff-914106e3d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save(df_llm, llm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3bee705-fc23-4c9c-91b8-332d9426e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark statistics for the LLM Models.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>resp_time_min</th>\n",
       "      <th>resp_time_max</th>\n",
       "      <th>resp_time_mean</th>\n",
       "      <th>out_tkn_min</th>\n",
       "      <th>out_tkn_max</th>\n",
       "      <th>out_tkn_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codellama13b</td>\n",
       "      <td>0.433</td>\n",
       "      <td>4.704</td>\n",
       "      <td>1.4712</td>\n",
       "      <td>151</td>\n",
       "      <td>1023</td>\n",
       "      <td>855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nv-llama2-70b-rlhf</td>\n",
       "      <td>0.143</td>\n",
       "      <td>4.709</td>\n",
       "      <td>2.0137</td>\n",
       "      <td>257</td>\n",
       "      <td>355</td>\n",
       "      <td>319.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>codellama70b</td>\n",
       "      <td>0.288</td>\n",
       "      <td>4.702</td>\n",
       "      <td>2.2310</td>\n",
       "      <td>569</td>\n",
       "      <td>1024</td>\n",
       "      <td>775.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral7binstruct</td>\n",
       "      <td>0.107</td>\n",
       "      <td>5.123</td>\n",
       "      <td>2.4344</td>\n",
       "      <td>136</td>\n",
       "      <td>599</td>\n",
       "      <td>369.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nemotron-3-8b-chat-steerlm</td>\n",
       "      <td>0.862</td>\n",
       "      <td>4.301</td>\n",
       "      <td>2.4845</td>\n",
       "      <td>279</td>\n",
       "      <td>608</td>\n",
       "      <td>414.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nv-llama2-70b-steerlm</td>\n",
       "      <td>0.116</td>\n",
       "      <td>4.395</td>\n",
       "      <td>2.5416</td>\n",
       "      <td>166</td>\n",
       "      <td>584</td>\n",
       "      <td>437.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codellama34b</td>\n",
       "      <td>0.296</td>\n",
       "      <td>4.818</td>\n",
       "      <td>2.6437</td>\n",
       "      <td>407</td>\n",
       "      <td>769</td>\n",
       "      <td>522.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama213b</td>\n",
       "      <td>0.199</td>\n",
       "      <td>4.971</td>\n",
       "      <td>2.7156</td>\n",
       "      <td>159</td>\n",
       "      <td>541</td>\n",
       "      <td>382.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yi-34b</td>\n",
       "      <td>0.214</td>\n",
       "      <td>4.864</td>\n",
       "      <td>2.7735</td>\n",
       "      <td>293</td>\n",
       "      <td>627</td>\n",
       "      <td>483.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama270b</td>\n",
       "      <td>0.307</td>\n",
       "      <td>5.119</td>\n",
       "      <td>2.9019</td>\n",
       "      <td>456</td>\n",
       "      <td>822</td>\n",
       "      <td>673.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mixtral8x7binstruct</td>\n",
       "      <td>1.530</td>\n",
       "      <td>5.130</td>\n",
       "      <td>3.7605</td>\n",
       "      <td>135</td>\n",
       "      <td>751</td>\n",
       "      <td>438.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name  resp_time_min  resp_time_max  resp_time_mean  \\\n",
       "0                 codellama13b          0.433          4.704          1.4712   \n",
       "1           nv-llama2-70b-rlhf          0.143          4.709          2.0137   \n",
       "2                 codellama70b          0.288          4.702          2.2310   \n",
       "3            mistral7binstruct          0.107          5.123          2.4344   \n",
       "4   nemotron-3-8b-chat-steerlm          0.862          4.301          2.4845   \n",
       "5        nv-llama2-70b-steerlm          0.116          4.395          2.5416   \n",
       "6                 codellama34b          0.296          4.818          2.6437   \n",
       "7                    llama213b          0.199          4.971          2.7156   \n",
       "8                       yi-34b          0.214          4.864          2.7735   \n",
       "9                    llama270b          0.307          5.119          2.9019   \n",
       "10         mixtral8x7binstruct          1.530          5.130          3.7605   \n",
       "\n",
       "    out_tkn_min  out_tkn_max  out_tkn_mean  \n",
       "0           151         1023         855.0  \n",
       "1           257          355         319.3  \n",
       "2           569         1024         775.5  \n",
       "3           136          599         369.2  \n",
       "4           279          608         414.4  \n",
       "5           166          584         437.6  \n",
       "6           407          769         522.8  \n",
       "7           159          541         382.8  \n",
       "8           293          627         483.8  \n",
       "9           456          822         673.7  \n",
       "10          135          751         438.9  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Benchmark statistics for the LLM Models.\")\n",
    "llm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a3f9ea-4156-40f8-a87e-a824af3b74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot of the LLM Results from multiple prompts.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>prompt</th>\n",
       "      <th>resp_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mixtral8x7binstruct</td>\n",
       "      <td>3.455</td>\n",
       "      <td>341</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>I'm here to provide you with accurate, helpful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral7binstruct</td>\n",
       "      <td>0.107</td>\n",
       "      <td>449</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>I'm glad you're asking about tourist attractio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nv-llama2-70b-rlhf</td>\n",
       "      <td>0.143</td>\n",
       "      <td>325</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Some of the top tourist attractions in Paris i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nv-llama2-70b-steerlm</td>\n",
       "      <td>1.727</td>\n",
       "      <td>584</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Paris is a city that is renowned for its rich ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codellama13b</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1023</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Paris is one of the most popular tourist desti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>codellama70b</td>\n",
       "      <td>0.610</td>\n",
       "      <td>828</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>ðŸ‡«ðŸ‡· Paris is a city of endless attractions, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>codellama34b</td>\n",
       "      <td>0.296</td>\n",
       "      <td>512</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Paris, the capital of France, is known as the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama213b</td>\n",
       "      <td>4.300</td>\n",
       "      <td>459</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Hello! As a helpful and respectful assistant, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama270b</td>\n",
       "      <td>5.119</td>\n",
       "      <td>722</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Paris, the capital of France, is known for its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yi-34b</td>\n",
       "      <td>0.814</td>\n",
       "      <td>336</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Paris is renowned for its rich history, stunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nemotron-3-8b-chat-steerlm</td>\n",
       "      <td>4.301</td>\n",
       "      <td>453</td>\n",
       "      <td>What are the top tourist attractions in Paris?</td>\n",
       "      <td>Paris is one of the most beautiful and histori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mixtral8x7binstruct</td>\n",
       "      <td>1.530</td>\n",
       "      <td>135</td>\n",
       "      <td>How does the Eiffel Tower contribute to the ci...</td>\n",
       "      <td>The Eiffel Tower significantly contributes to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mistral7binstruct</td>\n",
       "      <td>1.736</td>\n",
       "      <td>136</td>\n",
       "      <td>How does the Eiffel Tower contribute to the ci...</td>\n",
       "      <td>The Eiffel Tower is an iconic landmark and a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nv-llama2-70b-rlhf</td>\n",
       "      <td>4.709</td>\n",
       "      <td>310</td>\n",
       "      <td>How does the Eiffel Tower contribute to the ci...</td>\n",
       "      <td>The Eiffel Tower is an iconic landmark in Pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nv-llama2-70b-steerlm</td>\n",
       "      <td>4.395</td>\n",
       "      <td>451</td>\n",
       "      <td>How does the Eiffel Tower contribute to the ci...</td>\n",
       "      <td>The Eiffel Tower is an iconic landmark in Pari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name  resp_time  output_tokens  \\\n",
       "0          mixtral8x7binstruct      3.455            341   \n",
       "1            mistral7binstruct      0.107            449   \n",
       "2           nv-llama2-70b-rlhf      0.143            325   \n",
       "3        nv-llama2-70b-steerlm      1.727            584   \n",
       "4                 codellama13b      0.816           1023   \n",
       "5                 codellama70b      0.610            828   \n",
       "6                 codellama34b      0.296            512   \n",
       "7                    llama213b      4.300            459   \n",
       "8                    llama270b      5.119            722   \n",
       "9                       yi-34b      0.814            336   \n",
       "10  nemotron-3-8b-chat-steerlm      4.301            453   \n",
       "11         mixtral8x7binstruct      1.530            135   \n",
       "12           mistral7binstruct      1.736            136   \n",
       "13          nv-llama2-70b-rlhf      4.709            310   \n",
       "14       nv-llama2-70b-steerlm      4.395            451   \n",
       "\n",
       "                                               prompt  \\\n",
       "0      What are the top tourist attractions in Paris?   \n",
       "1      What are the top tourist attractions in Paris?   \n",
       "2      What are the top tourist attractions in Paris?   \n",
       "3      What are the top tourist attractions in Paris?   \n",
       "4      What are the top tourist attractions in Paris?   \n",
       "5      What are the top tourist attractions in Paris?   \n",
       "6      What are the top tourist attractions in Paris?   \n",
       "7      What are the top tourist attractions in Paris?   \n",
       "8      What are the top tourist attractions in Paris?   \n",
       "9      What are the top tourist attractions in Paris?   \n",
       "10     What are the top tourist attractions in Paris?   \n",
       "11  How does the Eiffel Tower contribute to the ci...   \n",
       "12  How does the Eiffel Tower contribute to the ci...   \n",
       "13  How does the Eiffel Tower contribute to the ci...   \n",
       "14  How does the Eiffel Tower contribute to the ci...   \n",
       "\n",
       "                                             resp_msg  \n",
       "0   I'm here to provide you with accurate, helpful...  \n",
       "1   I'm glad you're asking about tourist attractio...  \n",
       "2   Some of the top tourist attractions in Paris i...  \n",
       "3   Paris is a city that is renowned for its rich ...  \n",
       "4   Paris is one of the most popular tourist desti...  \n",
       "5   ðŸ‡«ðŸ‡· Paris is a city of endless attractions, but...  \n",
       "6   Paris, the capital of France, is known as the ...  \n",
       "7   Hello! As a helpful and respectful assistant, ...  \n",
       "8   Paris, the capital of France, is known for its...  \n",
       "9   Paris is renowned for its rich history, stunni...  \n",
       "10  Paris is one of the most beautiful and histori...  \n",
       "11  The Eiffel Tower significantly contributes to ...  \n",
       "12  The Eiffel Tower is an iconic landmark and a s...  \n",
       "13  The Eiffel Tower is an iconic landmark in Pari...  \n",
       "14  The Eiffel Tower is an iconic landmark in Pari...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Snapshot of the LLM Results from multiple prompts.\")\n",
    "df_llm.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee294dc4-644d-48ff-bfb4-cf4c15230daa",
   "metadata": {},
   "source": [
    "### Gradio Interface\n",
    "\n",
    "This section will present the functions of the LLMs as an interface through gradio.\n",
    "Users will select which model to use which allows them to enter a prompt to return \n",
    "the result.\n",
    "\n",
    "The function that invokes all the LLM will present the fastest models from top to bottom\n",
    "as a quick visual as to which model is best for the particular prompt.\n",
    "\n",
    "The third gradio interface is a simple file upload to look at the results that were saved\n",
    "as a json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9531cc8-a71a-421b-a1e0-7021eb295342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface_singular = gr.Interface(fn = llm_response_gradio,\n",
    "                     inputs =[gr.Radio(list(model_dict.keys()), label=\"Model Name\"),\n",
    "                              gr.Textbox(label=\"Enter your Prompt\")],\n",
    "                     outputs = gr.Textbox(label=\"LLM Output\"),\n",
    "                     title = \"Nvidia LLM Invoker\",\n",
    "                     description = \"Choose a model and enter a prompt to invoke a LLM model from Nvidia AI Foundational Models.\")\n",
    "\n",
    "iface_singular.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39834c71-8800-41ab-85ae-a89db93a93fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface_singular.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a336c3-28f8-4008-86a0-2146f34f6f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface_llm = gr.Interface(fn = llm_response_gradio_all,\n",
    "                         inputs = gr.Textbox(label=\"Enter your Prompt\"),\n",
    "                         outputs = gr.Textbox(label=\"Best Performing LLMs with respect to time.\"),\n",
    "                         title = \"Nvidia Multi-Model Invoker\",\n",
    "                         description = \"Enter a prompt to invoke multiple LLMs from Nvidia AI Foundational Models and return the response times.\"\n",
    ")\n",
    "\n",
    "iface_llm.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c9a9b93-1613-4cfd-aac7-398e554f44f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface_llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7f67f-dde3-494b-b204-192647339327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    return f.read()\n",
    "\n",
    "interface = gr.Interface(fn=read_file, inputs=\"file\", outputs=\"text\")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8168a-f647-4f70-a351-0e602645e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6005e838-aa25-4e97-9bc3-fc66fbb19a31",
   "metadata": {},
   "source": [
    "# NVIDIA AI Foundation Models Performance Analysis\n",
    "\n",
    "The Nvidia **foundational Large Language Models** (LLMs) are hosted on the catalog.ngc.nvidia.com webpage.\n",
    "Nvidia provides these models through either streaming or non-streaming process APIs for anyone\n",
    "to interact with many models. Nvidia provides an API key to developers to interact with these \n",
    "LLMs without a cost such as the OPENAI API and the rate-limiting of OPENAI. This allows new developers to\n",
    "experiment with LLMs with limited knowledge of the underlying architecture to obtain responses from \n",
    "prompts supplied.\n",
    "\n",
    "In this notebook, we will look at 11 models that are labeled text-to-text models on their catalog. Since\n",
    "all the APIs for the models are the same except the last path item which is the uuid for the model. These APIs\n",
    "will be called using the Python requests module. Other request modules such as async with aiohttp, \n",
    "and httpx.\n",
    "\n",
    "# Three main sections\n",
    "1.  Run a set of prompts through a set of models and collect statistics data (non-prompt type specific).\n",
    "2.  Run a set of 4 specific prompt scenarios (variation of input and output lengths) against all models and profile response behavior.\n",
    "3.  Run a Gradio interface to send a query to specified models chosen in the UI. \n",
    "\n",
    "\n",
    "#### NOTE\n",
    "There was experimentation with aiohttp with asyncio with the API calls. There are a few issues discovered:\n",
    "\n",
    "    1. Some models will return a 200 status code instead of the expected 202. This required a rewrite to account\n",
    "       for this problem.\n",
    "       \n",
    "    2. This problem is variable as it is not consistent with a particular model. If you want to async\n",
    "       call the API for each model for a single prompt, one to a few of the models will return with an empty\n",
    "       response. The amount of models that return an empty response will vary with each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e56ff7-f7c7-4ab0-b0bb-c4b9b3745e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os, sys\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd \n",
    "import gradio as gr\n",
    "from utils.llm_requests import ngc_request, ModelConfig, invoke_one_model, invoke_all_models\n",
    "from utils.format_response import format_resp_single, format_resp_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a271c-3d3e-4567-92c6-0e664329cb01",
   "metadata": {},
   "source": [
    "### Initialization \n",
    "In the cell below we will initialize our global variables that will be used by the LLM functions.\n",
    "\n",
    "#### NOTE:\n",
    "At the time of making this notebook, Mamba-chat had been released to the Nvidia catalog page on 02/12/2024.\n",
    "It was not included in the set of testing due to the payload differing in structure from the other which will\n",
    "require a rework to include.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254510f-913e-42ad-8a9e-1f1e61672f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGC Model file name (base folder should contain prompt csv, llm_models_file, and ngc_api_file.\n",
    "base_file_path = './data/'          # Folder that contains the below files\n",
    "llm_models_file = 'llm_models.yml'  # File containing all model names\n",
    "ngc_api_file = 'ngc-api.yml'        # File containing ngc url + key info\n",
    "csv_prompt_file = 'questions.csv'   # All prompts for section 1 \n",
    "json_prompt_file = 'kv_cache_test.json' # File that stores scenario prompts for section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743aa91-7923-41a3-ab03-e769a3ab65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the llm_models data\n",
    "with open(base_file_path + llm_models_file,'r') as f:\n",
    "    model_data = yaml.load(f, Loader=yaml.SafeLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a712d4c-0e3f-43c4-8e44-ce90f5142307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if a YAML file exists that contains your API key to NGC. \n",
    "# Read in the NGC API file. One can supplement their file in their data directory. \n",
    "with open(base_file_path + ngc_api_file, 'r') as file:\n",
    "    api_var = yaml.load(file, Loader = yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833865e-e48a-49a4-bdff-84e468c67d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging to inspect what type the data read from the yaml files.\n",
    "print(f\"model_data is a {type(model_data)}, and api_var is a {type(api_var)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd0a65-b52a-4f08-8100-6a966be408ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model URLs' dictionaries, model names, invoke URLs and API Key\n",
    "model_urls = model_data['model_urls']\n",
    "model_full_name = model_data['model_full_names'] # This variable is used for Gradio.\n",
    "fetch_url_format = model_data['invoke-urls'][1]\n",
    "my_api = api_var['NGCKEY']['API']\n",
    "\n",
    "# One can also initialize their API key below by uncommenting the code below:\n",
    "# os.environ['ngc_api'] = \"your-key-here\"\n",
    "# my_api = os.environ.get['ngc_api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89337ff-a67c-44b4-baf0-91d2900d58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# print(model_urls)\n",
    "# print(10*'-')\n",
    "# print(fetch_url_format)\n",
    "# print(my_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14ea60-ffcd-49e6-ac61-b6eaebe7ec1b",
   "metadata": {},
   "source": [
    "### LLM Functions\n",
    "Two main LLM functions interact with the Nvidia LLMs. All the models\n",
    "are interfacing with Nvidia Triton. Some of the models indicate what GPU/GPUS are\n",
    "used for the interfacing while some indicate others. Comparisons may not be \"apple-to-apple\"\n",
    "due to not all models interfacing with the same hardware.\n",
    "\n",
    "One function works by invoking a model name and a prompt to return the model\n",
    "response. This works with a helper function that formats the response to be displayed in a more readable format.\n",
    "This is all done by initializing the values into a class name \"ModelConfig\", model name, and model dictionary.\n",
    "\n",
    "The second function is similar to the first model saves the output in the list of dictionaries with a\n",
    "similar human-readable format. \n",
    "\n",
    "#### Testing the LLM invocation model\n",
    "In the next two cells below, one can call the function by supplying a model and prompt. The second cell\n",
    "allows the user just to specify a prompt and make the call to all models in the model dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899fe65-c033-4383-80bb-53cf62363b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the invocation of the NGC API to the models\n",
    "test_str = \"Describe the company, Nvidia.\"\n",
    "\n",
    "# Do one model call to test the connection\n",
    "test_model_name = \"nv-llama2-70b-rlhf\"\n",
    "\n",
    "# This class contains the prompt, API, temperature, top_p, max_tokens, seed, and stream configurations.\n",
    "test_model_config = ModelConfig(test_str, my_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dee834-479e-4474-bb81-73098025085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model that uses a single model and prompt. Time the response.\n",
    "s1_time = time.time()\n",
    "llm_response = format_resp_single(test_model_name, test_model_config, model_urls)\n",
    "s2_time = time.time()\n",
    "\n",
    "print(\"Total time taken: {} seconds. Note actual response time from AI foundations is used in analysis \\\n",
    "        \\n\".format(round((s2_time - s1_time),3)))\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d0479-23f9-4dd0-9813-eda0d6ce7f90",
   "metadata": {},
   "source": [
    "#### Doing a test run of a single prompt through all models specified in YAML file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7d195-77ad-4e1a-bf26-e8e5b81bc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function that uses a single prompt that iterates through all models. Time the response.\n",
    "s3_time = time.time()\n",
    "llm_all_resp = format_resp_all(test_model_config, model_urls)\n",
    "s4_time = time.time()\n",
    "\n",
    "print(\"Total time taken: {} minutes\\n \".format(round((s4_time-s3_time) / 60,3)))\n",
    "print(llm_all_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e5fe7-44cf-4df2-b377-5c0bc2b89385",
   "metadata": {},
   "source": [
    "# 1. Creating benchmark datasets - collect results for all models over all prompts\n",
    "\n",
    "The section below creates the LLM results dataframes and LLM benchmark results.\n",
    "\n",
    "The function **create_llm_dataset** generates a pandas dataframe when provided a csv or json file \n",
    "containing the input_prompt and model configurations for the eleven (11) text-to-text models.\n",
    "\n",
    "The function **create_stats_llm** generates a pandas dataframe from the dataframe created by\n",
    "create_llm_dataset that contains the statistical info of each model concerning response time\n",
    "and output_tokens.\n",
    "\n",
    "The function **df_save** saves the files created from the two functions above to a specified folder \n",
    "as json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe3822-b3f4-4713-aee7-07e9f8814d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_dataset(file_path:str, model_urls:dict) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Creates a data frame that is the product of running multiple\n",
    "    prompts through all the models and returns the response\n",
    "    message, model response time, and output tokens.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): CSV | JSON file path prompts and llm model configurations.\n",
    "\n",
    "    Outputs:\n",
    "        df_llm (pd.DataFrame): Output data frame of all prompts apply.\n",
    "                               to the function of calling all LLM models.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read in CSV prompt data and convert to list to process\n",
    "    if file_path.endswith('.csv'):\n",
    "        df_prompts = pd.read_csv(file_path, sep=\",\")\n",
    "    elif file_path.endswith('.json'):\n",
    "        df_prompts = pd.read_json(file_path)\n",
    " \n",
    "\n",
    "    lst_responses = [] # list to collect responses from all models (per prompt)\n",
    "    # Iterate through all prompts and get responses from all models\n",
    "    print(\"Starting LLM API calls\")\n",
    "    for counter, row in enumerate(df_prompts.itertuples()):\n",
    "        # Intialize the model configurations.\n",
    "        prompt_config = ModelConfig(row.questions, my_api, row.temperature, row.top_p, row.max_tokens, row.seed, row.stream)\n",
    "        \n",
    "        # Sending a single prompt to all selected models (output is a list of dictionaries)\n",
    "        full_llm_res = invoke_all_models(prompt_config, model_urls)\n",
    "        \n",
    "        # Collecting all responses from all models as a list of dictionaries for a single prompt.\n",
    "        lst_responses.append(full_llm_res)\n",
    "        print(\"Completed {} rounds\".format(counter+1))\n",
    "\n",
    "    \n",
    "    lst_pd = [] # list of responses for all prompts to be read into pandas\n",
    "    \n",
    "    # Decompose list of list of dictionaries to a single list of dictionaries for dataframe processing \n",
    "    for sublist in lst_responses:\n",
    "        for item in sublist:\n",
    "            lst_pd.append(item)\n",
    "\n",
    "    df_llm = pd.DataFrame(lst_pd) # Create the dataframe.\n",
    "    # Add the classification to each model run.\n",
    "    if file_path.endswith('.json'):\n",
    "        model_len = len(df_llm['model_name'].unique())\n",
    "        classes = ['short-in -> short-out'] * model_len + ['short-in -> long-out'] * model_len  + ['long-in -> short-out'] * model_len + ['long-in -> long-out'] * model_len\n",
    "        df_llm['class'] = classes\n",
    "\n",
    "    return df_llm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2d7a6-d106-462d-8930-17fd2035dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_llm(llm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a data frame that is a merge of two pandas groupby \n",
    "    dataframes of response time and output token.\n",
    "\n",
    "    Args: \n",
    "        llm_df (pd.DataFrame): Pandas dataframe of LLM responses.\n",
    "\n",
    "    Outputs:\n",
    "        stats_df (pd.DataFrame): Pandas dataframe of LLM aggregation stats.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Group by the response time for each model and take three statistical measures for response time.\n",
    "    time_df = llm_df.groupby(\"model_name\")['resp_time'].aggregate(resp_time_min='min',\n",
    "                                                                  resp_time_max='max',resp_time_mean='mean')\n",
    "    time_df = time_df.reset_index()\n",
    "    time_df = time_df.sort_values('resp_time_mean', ascending=True).reset_index(0, drop=True)\n",
    "\n",
    "    # Group by the output tokens for each model and take three statistical measures for output tokens.\n",
    "    tkn_df = llm_df.groupby(\"model_name\")['out_tokens'].aggregate(out_tkn_min=\"min\", out_tkn_max=\"max\", out_tkn_mean=\"mean\")\n",
    "    tkn_df = tkn_df.reset_index()\n",
    "    tkn_df = tkn_df.sort_values(\"out_tkn_mean\", ascending=True).reset_index(0, drop=True)\n",
    "\n",
    "    # Merge both dataframes into one dataframe for ease of use.\n",
    "    stats_df = time_df.merge(tkn_df, how=\"inner\", on=\"model_name\")\n",
    "\n",
    "    return stats_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b7503-340c-42e9-8405-456832890a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_save(llm_df: pd.DataFrame, stats_llm: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Saves two data frames to separate json files.\n",
    "\n",
    "    Args:\n",
    "        llm_df (pd.DataFrame): pandas dataframe of LLM responses\n",
    "        stats_llm (pd.DataFrame): pandas dataframe of LLM aggregation stats.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Create a folder to store the outputs.\n",
    "    folder_path = \"./output_llm/\"\n",
    "    os.makedirs(folder_path, exist_ok = True)\n",
    "\n",
    "    # Saving the inputs from llm_results and stats_llm.\n",
    "    llm_df.to_json(folder_path + \"results_llm.json\", orient = \"records\", compression = \"infer\")\n",
    "\n",
    "    stats_llm.to_json(folder_path + \"stats_llm.json\", orient = \"records\", compression = \"infer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba84260-74e5-409a-bbfb-fc1b3a3b45bc",
   "metadata": {},
   "source": [
    "### Sending the prompt request for all prompt data over all models \n",
    "Below this cell, it will call the above dataframe functions to create the \n",
    "data structure we need to get the statistical info concerning the models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ac11d-6de5-4695-9766-3ade89884ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file has five rows of prompt data.\n",
    "csv_path = base_file_path + csv_prompt_file # Setting the csv path.\n",
    "\n",
    "start_tm = time.time() \n",
    "# Create the LLM results dataframe. (#prompts x #models)\n",
    "df_llm = create_llm_dataset(csv_path, model_urls)\n",
    "\n",
    "# Create the Statistical LLM dataframe. (#models x 6-statistical measures)\n",
    "llm_stats = create_stats_llm(df_llm)\n",
    "\n",
    "end_tm = time.time() \n",
    "elapsed_time = (end_tm - start_tm) / 60\n",
    "print(\"Elapsed time to run benchmark: {} minutes\".format(round(elapsed_time,3)))\n",
    "\n",
    "# Save the data structures to json.\n",
    "df_save(df_llm, llm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bee705-fc23-4c9c-91b8-332d9426e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the benchmark dataframe for the LLM models.\n",
    "print(\"Benchmark statistics for the LLM Models.\")\n",
    "print(f\"Shape of llm_stats dataframe is {llm_stats.shape}\")\n",
    "llm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3f9ea-4156-40f8-a87e-a824af3b74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Snapshot of the LLM Results from multiple prompts.\")\n",
    "df_llm.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c60a0ab-9b09-4b81-b35d-490c9df7c31e",
   "metadata": {},
   "source": [
    "# 2. Prompt scenario performance experiment.\n",
    "\n",
    "Now, it is time to the feed json data containing four scenarios pretraining to the length of the input and output tokens.\n",
    "The scenarios are listed as follows:\n",
    " - short input -> short output\n",
    " - short input -> long output\n",
    " - long input -> short output\n",
    " - long input -> long output\n",
    "\n",
    "**NOTE**: There appears to be an issue if max_tokens is set above 1024, it returns a 422 HTTP Error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86272d0-7b75-40ae-8052-800418b2c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize json file path. This file contains four JSON items.\n",
    "json_path = base_file_path + json_prompt_file\n",
    "\n",
    "# Create the LLM results dataframe for the scenarios. Dataframe size is (4 X #models) x 6 data columns \n",
    "start_time = time.time()\n",
    "df_llm_kv = create_llm_dataset(json_path, model_urls)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time_f = (end_time - start_time) / 60\n",
    "print(\"Elapsed time to run benchmark: {} minutes\".format(round(elapsed_time_f,3)))\n",
    "print(\"Rows: {}, Columns: {}\".format(df_llm_kv.shape[0], df_llm_kv.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc50ab4-733b-4342-81ec-4585f43c826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataframe to check for the validity of the dataframe transformation.\n",
    "df_llm_kv.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c2878-02ea-45c1-b6c1-03d17fbb395b",
   "metadata": {},
   "source": [
    "# Charts of the results.\n",
    "Let us take a look at the results through Matplotlib. The cells below will display seven\n",
    "different plots of the response time vs input and output tokens and complete tokens (input and output tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afa1f4-c28c-40ef-bb83-822db6759899",
   "metadata": {},
   "source": [
    "### Initialize plots variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b2968-901c-4764-98f9-2977f8e5f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataframe to a variable to use in plotting.\n",
    "df = df_llm_kv\n",
    "\n",
    "## Defining plot values\n",
    "markers = [\n",
    "    \"o\",  # Circle\n",
    "    \"s\",  # Square\n",
    "    \"^\",  # Triangle Up\n",
    "    \"P\",  # Plus (filled)\n",
    "    \"*\",  # Star\n",
    "    \"$\\clubsuit$\",  # Club symbol using LaTeX\n",
    "    \"X\",  # X (filled)\n",
    "    \"D\",  # Diamond\n",
    "    \"p\",  # Pentagon\n",
    "    \"H\",  # Hexagon\n",
    "    \"v\",  # Triangle Down\n",
    "    \"+\"   # Plus\n",
    "]\n",
    "\n",
    "marker_size = 100  # Size of markers\n",
    "\n",
    "# Set the colors magenta, light green, orangered, and blue into a numpy array\n",
    "colors = np.array([[1.0, 0.0, 1.0, 1.0],\n",
    "                   [0.0, 0.5, 0.0, 1.0],\n",
    "                   [1.0, 0.27, 0.0, 1.0],\n",
    "                   [0.0, 0.0, 1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Unique values for model_name and class\n",
    "unique_models = df['model_name'].unique()\n",
    "unique_classes = df['class'].unique()\n",
    "\n",
    "# Markers and colors to be used in the subplots.\n",
    "m_subplots = markers[:len(unique_models)]\n",
    "c_subplots = ['magenta', 'green', 'orangered', 'blue']\n",
    "\n",
    "# Line style and width for grid lines.\n",
    "line_style, line_width = \"--\", 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956257f9-c897-41b7-acc4-aff47851fc3a",
   "metadata": {},
   "source": [
    "## Subplots of Output Tokens vs Response Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f50e6-c2ca-414a-b55f-5ca9cf1a0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each class for plotting\n",
    "for i, class_value in enumerate(unique_classes):\n",
    "    ax = axs[i]\n",
    "    subset = df[df['class'] == class_value]\n",
    "    \n",
    "    # Plot each model with a different marker\n",
    "    for j, model in enumerate(unique_models):\n",
    "        model_subset = subset[subset['model_name'] == model]\n",
    "        ax.scatter(model_subset['out_tokens'], model_subset['resp_time'], \n",
    "                   marker=m_subplots[j], color=c_subplots[i], label=model, s=marker_size)\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_title(f'Query Type: {class_value}', fontsize=14)\n",
    "    ax.grid(axis='both', linewidth = line_width, linestyle = line_style)\n",
    "    ax.set_xlabel('Number of Output Tokens')\n",
    "    ax.set_ylabel('Response Time secs')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker=markers[i], color='w', label=unique_models[i],\n",
    "                          markerfacecolor='k', markersize=12) for i in range(len(unique_models))]\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.1, 1), title='Model')\n",
    "plt.savefig('output_llm/response_time-vs-out_tokens-subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f3343-410c-491f-a010-e38781833f2c",
   "metadata": {},
   "source": [
    "## Subplots of Input Tokens vs Response Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cebeb7-feaa-4634-ac1d-a07f35a98c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each class for plotting\n",
    "for i, class_value in enumerate(unique_classes):\n",
    "    ax = axs[i]\n",
    "    subset = df[df['class'] == class_value]\n",
    "    \n",
    "    # Plot each model with a different marker\n",
    "    for j, model in enumerate(unique_models):\n",
    "        model_subset = subset[subset['model_name'] == model]\n",
    "        ax.scatter(model_subset['in_tokens'], model_subset['resp_time'], \n",
    "                   marker=m_subplots[j], color=c_subplots[i], label=model, s=marker_size)\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_title(f'Query Type: {class_value}', fontsize=14)\n",
    "    ax.grid(axis='both', linewidth = line_width, linestyle = line_style)\n",
    "    ax.set_xlabel('Number of Input Tokens')\n",
    "    ax.set_ylabel('Response Time secs')\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker=markers[i], color='w', label=unique_models[i],\n",
    "                          markerfacecolor='k', markersize=12) for i in range(len(unique_models))]\n",
    "\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.1, 1), title='Model')\n",
    "plt.savefig('output_llm/response_time-vs-in_tokens-subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ed850-a136-436b-acb2-ab48c1acecab",
   "metadata": {},
   "source": [
    "## Response time vs number of input tokens across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa5111-9d36-405c-a2a8-5ed0d0d60643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8)) # Create a scatter plot\n",
    "\n",
    "# Loop over each unique model_name\n",
    "for i, model in enumerate(df['model_name'].unique()):\n",
    "    # Subset the DataFrame based on model_name\n",
    "    subset = df[df['model_name'] == model]\n",
    "    \n",
    "    # Loop over each unique prompt_id within the subset\n",
    "    for j, prompt_id in enumerate(subset['class'].unique()):\n",
    "        prompt_subset = subset[subset['class'] == prompt_id]\n",
    "        \n",
    "        # Plot using a different marker and color for each prompt_id\n",
    "        plt.scatter(prompt_subset['in_tokens'], prompt_subset['resp_time'], label=f\"{model} - query-type: {prompt_id}\",\n",
    "                    marker=markers[i % len(markers)], color=colors[j % len(colors)], s=marker_size)\n",
    "\n",
    "# Labeling axes and title\n",
    "plt.xlabel('Number of Input Tokens')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "plt.title('Response Time vs. Number of Input Tokens Across All Models')\n",
    "\n",
    "# Place the legend outside the plot on the right side\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title='Model - Prompt Type')\n",
    "plt.grid(axis='x', linewidth = 0.5, linestyle = '--')\n",
    "plt.grid(axis='y', linewidth = 0.5, linestyle = '--')\n",
    "plt.tight_layout()  # Adjust layout to not cut off the legend\n",
    "plt.savefig('output_llm/response_time-vs-in_tokens-all.png') # Save the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552db241-1d98-4eeb-a821-5d9bf7b5f9ea",
   "metadata": {},
   "source": [
    "# Response time vs number of output tokens across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54eb42a-b070-4bea-a5c0-2ff2c81937b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop over each unique model_name\n",
    "for i, model in enumerate(df['model_name'].unique()):\n",
    "    # Subset the DataFrame based on model_name\n",
    "    subset = df[df['model_name'] == model]\n",
    "    \n",
    "    # Loop over each unique prompt_id within the subset\n",
    "    for j, prompt_id in enumerate(subset['class'].unique()):\n",
    "        prompt_subset = subset[subset['class'] == prompt_id]\n",
    "        \n",
    "        # Plot using a different marker and color for each prompt_id\n",
    "        plt.scatter(prompt_subset['out_tokens'], prompt_subset['resp_time'], label=f\"{model} - query-type {prompt_id}\",\n",
    "                    marker=markers[i % len(markers)], color=colors[j % len(colors)], s=marker_size)\n",
    "\n",
    "# Labeling axes and title\n",
    "plt.xlabel('Number of Output Tokens')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "plt.title('Response Time vs. Number of Output Tokens Across All Models')\n",
    "\n",
    "# Place the legend outside the plot on the right side\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title='Model - Prompt Type')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to not cut off the legend\n",
    "plt.grid(axis='x', linewidth = 0.5, linestyle = '--')\n",
    "plt.grid(axis='y', linewidth = 0.5, linestyle = '--')\n",
    "plt.savefig('output_llm/response_time-vs-out_tokens-all.png') # Save the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee294dc4-644d-48ff-bfb4-cf4c15230daa",
   "metadata": {},
   "source": [
    "# 3. Gradio Interface\n",
    "\n",
    "This section will present the functions of the LLMs as an interface through radio.\n",
    "\n",
    "1. Users will select which model to use which allows them to enter a prompt to return \n",
    "   the result.\n",
    "\n",
    "2. The function that invokes all the LLM will present the fastest models from top to bottom\n",
    "   as a quick visual as to which model is best for the particular prompt.\n",
    "\n",
    "**NOTE**: Functions had to be re-written to work with Gradio so that there are two functions in the notebook \n",
    "          that function similarly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0130c5-8638-45da-9b84-d47e4f81d0fe",
   "metadata": {},
   "source": [
    "### Gradio Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a972a-587c-470b-9efc-585e402bb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_invoke_all(prompt:str):\n",
    "  \"\"\"\n",
    "    This function will call from any model within the dictionary from Nvidia\n",
    "    AI foundational models and run the model. This is strictly focused on\n",
    "    the text-to-text models found on the link below:\n",
    "\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
    "\n",
    "    Models that require context are not included in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        prompt (str) -> Prompt to be passed to the model.\n",
    "\n",
    "    Outputs\n",
    "        msg (str) -> text generated from the model given the prompt.\n",
    "        resp_time (str) -> time taken to generate the response.\n",
    "        input_tokens (int) -> number of tokens generated by the user input.\n",
    "        out_tokens (int) -> number of tokens produced by the LLM.\n",
    "        model_name (str) -> name of the model that was called by the curl request.\n",
    "        prompt (str) -> prompt used to generate the output.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  headers = {\n",
    "    \"Authorization\": \"Bearer \" + str(my_api),\n",
    "    \"Accept\": \"application/json\",\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": str(prompt),\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"seed\": 42,\n",
    "  \"stream\": False\n",
    "  }\n",
    "\n",
    "  lst_resp = [] # Initialize list for the main output.\n",
    "    \n",
    "  #Create session.\n",
    "  session = requests.Session()\n",
    "\n",
    "  # Issue: function had to be rewritten to solve None results from output_tokens.   \n",
    "  for key, url in model_urls.items():\n",
    "    tmp_dict = {}\n",
    "    response = session.post(url, headers = headers, json = payload)\n",
    "\n",
    "    while response.status_code == 202:\n",
    "      request_id = response.headers.get(\"NVCF-REQID\")\n",
    "      fetch_url = fetch_url_format + request_id\n",
    "      response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "    response_body = response.json()\n",
    "    msg = response_body.get('choices')[0].get('message').get('content')\n",
    "    resp_time = round(response.elapsed.total_seconds(), 3)\n",
    "    out_tokens = response_body.get('usage').get('completion_tokens')\n",
    "    in_tokens = response_body.get('usage').get('prompt_tokens')\n",
    "    tmp_dict = {\"model_name\": key,\"resp_time\": resp_time, \"out_tokens\": out_tokens,\n",
    "                \"in_tokens\": in_tokens, \"prompt\": prompt, \"resp_msg\": msg}\n",
    "    lst_resp.append(tmp_dict)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "  return lst_resp\n",
    "\n",
    "def llm_invoke(model_name:str, prompt:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will call from any model within the dictionary from Nvidia\n",
    "    AI foundational models and run the model. This is strictly focused on\n",
    "    the text-to-text models found on the link below:\n",
    "\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
    "\n",
    "    Models that require context are not included in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): name of the model\n",
    "        prompt (str): prompt to be passed to the model\n",
    "\n",
    "    Outputs:\n",
    "    Dictionary of the following outputs.\n",
    "        model_name (str): name of the model\n",
    "        resp_time (str): time taken to generate the response\n",
    "        out_tokens (int): number of tokens returned from the LLM.\n",
    "        in_tokens (int): number of tokens that represent the prompt.\n",
    "        prompt (str): input string to be ingested by the LLM.\n",
    "        msg (str): text generated from the model given the prompt\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize the headers and payloads for the CURL requests.\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + str(my_api),\n",
    "        \"Accept\": \"application/json\",\n",
    "     }\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "        {\n",
    "            \"content\": str(prompt),\n",
    "             \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"seed\": 42,\n",
    "    \"stream\": False\n",
    "    }\n",
    "\n",
    "    # Error catching if not model is not list of models.\n",
    "    model_name = model_name.lower().replace(\" \",\"\")\n",
    "    if model_name not in model_urls.keys():\n",
    "        print(\"Model name not found in dictionary, using default model\")\n",
    "        print(\"Default model is NV-Llama2-70B-RLHF\")\n",
    "        model_name = \"nv-llama2-70b-rlhf\"\n",
    "    model_url = model_urls[model_name]\n",
    "    \n",
    "    model_session = requests.Session()\n",
    "    \n",
    "    return ngc_request(model_session, model_url, headers, payload, model_name, prompt)\n",
    "\n",
    "def gradio_all(prompt: str, save_as_file=False):\n",
    "    \"\"\"\n",
    "    Formats the output to human readable style when a prompt is sent\n",
    "    to all models\n",
    "\n",
    "    Args\n",
    "        prompt: string input from the user.\n",
    "        save_as_file: boolean for the user to click checkbox to save the file.\n",
    "    \"\"\"\n",
    "    content_lst = list() # Initialize an empty list.\n",
    "    response_lst = llm_invoke_all(prompt) # Call the function. \n",
    "    response_lst = sorted(response_lst, key=lambda x: x['resp_time'], reverse = False) # Sort by response time.\n",
    "    save_json(response_lst, save_as_file) # Call the save file function.\n",
    "    for doc in response_lst:\n",
    "        content = f\"Model Name: {doc['model_name']}  Response Time:{doc['resp_time']} seconds\\n  #Input Tokens:{doc['in_tokens']}  #Output Tokens:{doc['out_tokens']}\\n\\n\"\n",
    "        content_lst.append(content)\n",
    "    return \" \".join(content_lst)\n",
    "\n",
    "def save_json(data, save_as_file):\n",
    "    \"\"\"\n",
    "    Gives the option in Gradio to save output to JSON.\n",
    "\n",
    "    Args\n",
    "        data (dict | list): either a dictionary or a list of dictionaries\n",
    "        save_as_file (bool): boolean for the user to click the checkbox to save the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    json_data = json.dumps(data)\n",
    "    if save_as_file:\n",
    "        with open('./data/response_llm.json', 'w') as file:\n",
    "            file.write(json_data)\n",
    "        return \"File Saved Successfully\"\n",
    "    else:\n",
    "        return json_data\n",
    "\n",
    "def gradio_one(model_name:str, prompt:str, save_as_file=False):\n",
    "    \"\"\"\n",
    "    Formats the output to a human-readable style when a model is selected\n",
    "    get a generated output.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): model name that is being invoked\n",
    "        prompt (str): string containing the human input text.\n",
    "        save_as_file (bool): boolean for the user to click the xcheckbox to save the file.\n",
    "    \"\"\"\n",
    "    # Call the model and format the output by calling the key values.\n",
    "    llm_out = llm_invoke(model_name, prompt)\n",
    "    \n",
    "    output = f\"{llm_out['message']}\\n\\nResponse time: {llm_out['resp_time']} seconds\\n\\n #Output Tokens:{llm_out['out_tokens']}  #Input Tokens:{llm_out['in_tokens']}\\n  Model Name:{llm_out['model_name']}\"\n",
    "    save_json(llm_out, save_as_file)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81067c33-a96f-42de-99e3-d661a65d3d92",
   "metadata": {},
   "source": [
    "### Gradio Interface: Single Model with prompt with option to save result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9531cc8-a71a-421b-a1e0-7021eb295342",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface_singular = gr.Interface(fn = gradio_one,\n",
    "                     inputs =[gr.Radio(list(model_full_name.values()), label=\"Model Name\"),\n",
    "                              gr.Textbox(label=\"Enter your Prompt\"),\n",
    "                              gr.Checkbox(label=\"Save as File\")],\n",
    "                     outputs = gr.Textbox(label=\"LLM Output\"),\n",
    "                     title = \"Nvidia LLM Invoker\",\n",
    "                     description = \"Choose a model and enter a prompt to invoke a LLM model from Nvidia AI Foundational Models.\")\n",
    "\n",
    "iface_singular.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39834c71-8800-41ab-85ae-a89db93a93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface_singular.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76685f26-75dc-4bbc-98ff-9a4633f37aca",
   "metadata": {},
   "source": [
    "### Gradio Interface: Single Prompt across all models with option to save result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a336c3-28f8-4008-86a0-2146f34f6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface_llm = gr.Interface(fn = gradio_all,\n",
    "                         inputs = [gr.Textbox(label=\"Enter your Prompt\"),\n",
    "                                   gr.Checkbox(label=\"Save as file\")],\n",
    "                         outputs = gr.Textbox(label=\"Best Performing LLMs with respect to time.\"),\n",
    "                         title = \"Nvidia Multi-Model Invoker\",\n",
    "                         description = \"Enter a prompt to invoke multiple LLMs from Nvidia AI Foundational Models and return the response times.\"\n",
    ")\n",
    "\n",
    "iface_llm.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a9b93-1613-4cfd-aac7-398e554f44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface_llm.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76875aa-4b64-4659-a903-818a366ca12c",
   "metadata": {},
   "source": [
    "# Conclusion and Additonal Work.\n",
    "\n",
    "In this notebook, we were able to test multiple models from the NGC AI foundational models catalog against many prompts.\n",
    "We were able to plot the results of the test to see how models perform in different scenarios as well as benchmark models\n",
    "using statistical measures to see which were the best-performing models concerning response time.\n",
    "\n",
    "Here is some additional work that can be done to improve this notebook:\n",
    "\n",
    "* Refactor code so that the request function to send prompts to NGC models works with or without Gradio. The current code had to be separated into two separate functions that perform the same function.\n",
    "\n",
    "* Extend the functionality to other types of models on the NVIDIA AI Foundation Model catalog. This notebook focused on text-to-text models where the model payloads were of the same structure.\n",
    "\n",
    "* Add functionality to allow for async CURL requests to the API. Encountered two bugs when working the modules async and aiohttp. The explanation can be found at the top of the notebook.\n",
    "\n",
    "* Extend the functionality to evaluate the metrics of each LLM for accuracy to different topics. Each request gives back a text response. Evaluating the speed of an LLM is easy, but accuracy is not and it is very dependent on the use case. Possibly leverage [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a53237-6e39-405c-8235-27884ba8ba6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6005e838-aa25-4e97-9bc3-fc66fbb19a31",
   "metadata": {},
   "source": [
    "# Nvidia Foundational Large Language Models\n",
    "\n",
    "The Nvidia **foundational Large Language Models** (LLMs) are hosted on the catalog.ngc.nvidia.com webpage.\n",
    "Nvidia provides these models through either streaming or non-streaming process APIs for anyone\n",
    "to interact with many models. Nvidia provides an API key to developers to interact with these \n",
    "LLMs without a cost such as the OPENAI API and the rate-limiting of OPENAI. This allows new developers to\n",
    "experiment with LLMs with limited knowledge of the underlying architecture to obtain responses from \n",
    "prompts supplied.\n",
    "\n",
    "In this notebook, we will look at 11 models that are labeled text-to-text models on their catalog. Since\n",
    "all the APIs for the models are the same except the last path item which is the uuid for the model. These APIs\n",
    "will be called using the Python requests module. Other request modules such as async with aiohttp, \n",
    "and httpx.\n",
    "\n",
    "#### NOTE\n",
    "There was experimentation with aiohttp with asyncio with the API calls. There are few issues discovered:\n",
    "\n",
    "    1. Some models will return a 200 status code instead of the expected 202. This required a rewrite to account\n",
    "       for this problem.\n",
    "       \n",
    "    2. This problem is variable in nature as it is not consistent to a particular model. If you want to async\n",
    "       call the API for each model for a single prompt, one to a few of the models will return with an empty\n",
    "       response. The amount of models that return an empty response will vary with each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "50e56ff7-f7c7-4ab0-b0bb-c4b9b3745e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import os, sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd \n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a271c-3d3e-4567-92c6-0e664329cb01",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "In the cell below we will initialize our global variables that wil be used by the LLM functions.\n",
    "\n",
    "#### NOTE:\n",
    "At the time of making this notebook, Mamba-chat had been released to the Nvidia catalog page on 02/12/2024.\n",
    "It was not included in the set of testing due to the payload differing in structure from the other whcih will\n",
    "require a rework to include.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d20114-ba03-4923-a7ab-63ef4cae03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"mixtral8x7binstruct\": \"8f4118ba-60a8-4e6b-8574-e38a4067a4a3\",\n",
    "    \"mistral7binstruct\": \"35ec3354-2681-4d0e-a8dd-80325dcf7c63\",\n",
    "    \"nv-llama2-70b-rlhf\": \"7b3e3361-4266-41c8-b312-f5e33c81fc92\",\n",
    "    \"nv-llama2-70b-steerlm\": \"d6fe6881-973a-4279-a0f8-e1d486c9618d\",\n",
    "    \"codellama13b\": \"f6a96af4-8bf9-4294-96d6-d71aa787612e\",\n",
    "    \"codellama70b\": \"2ae529dc-f728-4a46-9b8d-2697213666d8\",\n",
    "    \"codellama34b\": \"df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8\",\n",
    "    \"llama213b\": \"e0bb7fb9-5333-4a27-8534-c6288f921d3f\",\n",
    "    \"llama270b\": \"0e349b44-440a-44e1-93e9-abe8dcb27158\",\n",
    "    \"yi-34b\": \"347fa3f3-d675-432c-b844-669ef8ee53df\",\n",
    "    \"nemotron-3-8b-chat-steerlm\": \"1423ff2f-d1c7-4061-82a7-9e8c67afd43a\",\n",
    "}\n",
    "\n",
    "invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/\"\n",
    "fetch_url_format = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/status/\"\n",
    "\n",
    "model_urls = {}\n",
    "for k, v in model_dict.items():\n",
    "    model_urls[k] = invoke_url + v\n",
    "\n",
    "# This method is not the safest way to initialize an API key. This is meant for demonstration.\n",
    "# Insert your ngc catalog API key which can be found on the model's catalog page.\n",
    "os.environ[\"NGCKEY\"] = \"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\"\n",
    "my_api = os.environ.get('NGCKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f2d6f-3f23-4ce4-85fe-cc30e85a3cd1",
   "metadata": {},
   "source": [
    "### LLM Functions\n",
    "Two main LLM functions interact with the Nvidia LLMs. All the models\n",
    "are interfacing with Nvidia Triton. Some of the models indicate what GPU/GPUS are\n",
    "used for the interfacing while some indicate others. Comparisons may not be \"apple-to-apple\"\n",
    "due to not all models interfacing with the same hardware.\n",
    "\n",
    "One function named \"llm_invoke\" works by invoking a model name and prompt to return the model\n",
    "response. This works with a helper function that formats the response to be displayed in the gradio\n",
    "interface.\n",
    "\n",
    "The second function is similar to the first model saves the output in the list of dictionaries with a\n",
    "similar gradio formatting function supplementing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a986ec47-cb9d-401c-b985-d32487a82b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_invoke(model_name:str, prompt:str):\n",
    "  \"\"\"\n",
    "    This function will call from any model within the dictionary from Nvidia\n",
    "    AI foundational models and run the model. This is strictly focused on\n",
    "    the text-to-text models found on the link below:\n",
    "\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
    "\n",
    "    Models that require context are not included in the dictionary.\n",
    "\n",
    "    Inputs\n",
    "    model_name (str): name of the model\n",
    "    prompt (str): prompt to be passed to the model\n",
    "\n",
    "    Outputs\n",
    "    msg (str): text generated from the model given the prompt\n",
    "    resp_time (str): time taken to generate the response\n",
    "    out_Tokens (str): Number of tokens returned from the LLM.\n",
    "\n",
    "  \"\"\"\n",
    "  model_name = model_name.lower().replace(\" \", \"\")\n",
    "\n",
    "  headers = {\n",
    "    \"Authorization\": \"Bearer \" + str(my_api),\n",
    "    \"Accept\": \"application/json\",\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": str(prompt),\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"seed\": 42,\n",
    "  \"stream\": False\n",
    "  }\n",
    "\n",
    "  if model_name not in model_dict.keys():\n",
    "    print(\"Model name not found in dictionary, using default model\")\n",
    "    print(\"Default model is NV-Llama2-70B-RLHF\")\n",
    "    model_name = \"nv-llama2-70b-rlhf\"\n",
    "\n",
    "  #Create session.\n",
    "  session = req.Session()\n",
    "\n",
    "  response = session.post(model_urls[model_name], headers=headers, json=payload)\n",
    "\n",
    "  while response.status_code == 202:\n",
    "    request_id = response.headers.get(\"NVCF-REQID\")\n",
    "    fetch_url = fetch_url_format + request_id\n",
    "    response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "  response.raise_for_status()\n",
    "  response_body = response.json()\n",
    "  msg = response_body.get('choices')[0].get('message').get('content')\n",
    "  resp_time = round(response.elapsed.total_seconds(), 3)\n",
    "  out_tokens = response_body.get('usage').get('completion_tokens')\n",
    "  return msg, resp_time, out_tokens\n",
    "\n",
    "def llm_response_gradio(model_name: str, prompt: str):\n",
    "    msg, resp_time, out_tokens = llm_invoke(model_name, prompt)\n",
    "    output = f\"{msg}\\n\\nResponse time: {resp_time} seconds\\n\\nOutput_tokens:{out_tokens}\"\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba04eb7-c707-4e57-bab4-ade99f0c635e",
   "metadata": {},
   "source": [
    "Testing the llm invocation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c6ec7841-25e3-45ec-8d7a-3fea5b264907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Result:\n",
      "Paris is a city with a rich history, art, architecture, and culture. Here are five of the best spots to visit in Paris:\n",
      "1. The Eiffel Tower: This iconic tower is one of the most recognizable landmarks in the world. Visitors can take the elevator to the top of the tower for panoramic views of the city.\n",
      "2. The Louvre Museum: This world-renowned museum is home to some of the most famous and iconic works of art in the world, including the Mona Lisa. Visitors can take a guided tour of the museum to learn more about the history and significance of the works of art on display.\n",
      "3. The Arc de Triomphe: This iconic monument is one of the most recognizable landmarks in the world. Visitors can take a guided tour of the monument to learn more about its history and significance.\n",
      "4. The Notre Dame Cathedral: This world-renowned cathedral is one of the most recognizable landmarks in the world. Visitors can take a guided tour of the cathedral to learn more about its history and significance.\n",
      "5. The Champs Elysees: This iconic street is one of the most recognizable landmarks in the world. Visitors can take a guided tour of the street to learn more about its history and significance.\n",
      "\n",
      "Response time: 1.015 seconds\n",
      "\n",
      "Output_tokens:1023\n"
     ]
    }
   ],
   "source": [
    "test_prompt_singular = \"I am visiting paris, what should I see? Limit to five best spots.\"\n",
    "result_singular = llm_response_gradio(\"codellama13b\", test_prompt_singular)\n",
    "print(\"LLM Result:\\n{}\".format(result_singular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d47b7a15-7634-48b2-ab99-2e79496741c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_invoke_all(prompt:str):\n",
    "  \"\"\"\n",
    "    This function will call from any model within the dictionary from Nvidia\n",
    "    AI foundational models and run the model. This is strictly focused on\n",
    "    the text to text models found on the link below:\n",
    "\n",
    "    https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models\n",
    "\n",
    "    Models that required context are not included in the dictionary.\n",
    "\n",
    "    Inputs\n",
    "    prompt -> Prompt to be passed to the model\n",
    "\n",
    "    Outputs\n",
    "    msg -> Text generated from the model given the prompt\n",
    "    Resp_time -> Time taken to generate the response\n",
    "    out_tokens -> Number of tokens produced by the LLM\n",
    "    model_name -> Name of the model that was called by the curl request.\n",
    "\n",
    "  \"\"\"\n",
    "  lst_resp = []\n",
    "  headers = {\n",
    "    \"Authorization\": \"Bearer \" + str(my_api),\n",
    "    \"Accept\": \"application/json\",\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": str(prompt),\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"seed\": 42,\n",
    "  \"stream\": False\n",
    "  }\n",
    "\n",
    "  #Create session.\n",
    "  session = req.Session()\n",
    "\n",
    "  for key, url in model_urls.items():\n",
    "    tmp_dict = {}\n",
    "    response = session.post(url, headers = headers, json = payload)\n",
    "\n",
    "    while response.status_code == 202:\n",
    "      request_id = response.headers.get(\"NVCF-REQID\")\n",
    "      fetch_url = fetch_url_format + request_id\n",
    "      response = session.get(fetch_url, headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "    response_body = response.json()\n",
    "    msg = response_body.get('choices')[0].get('message').get('content')\n",
    "    resp_time = round(response.elapsed.total_seconds(), 3)\n",
    "    out_tokens = response_body.get('usage').get('completion_tokens')\n",
    "    tmp_dict = {\"model_name\": key,\"resp_time\": resp_time, \"output_tokens\": out_tokens,\n",
    "                \"prompt\": prompt, \"resp_msg\": msg}\n",
    "    lst_resp.append(tmp_dict)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "  return lst_resp\n",
    "\n",
    "def llm_response_gradio_all(prompt: str):\n",
    "    content_lst = list()\n",
    "    response_lst = llm_invoke_all(prompt)\n",
    "    response_lst = sorted(response_lst, key=lambda x: x['resp_time'], reverse = False)\n",
    "    for doc in response_lst:\n",
    "        content = f\"Model:{doc.get('model_name')}\\n\\nResponse Time:{doc.get('resp_time')}\\n\\nTokens Produced:{doc.get('output_tokens')}\\n\\n\"\n",
    "        content_lst.append(content)\n",
    "    return \" \".join(content_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e5fe7-44cf-4df2-b377-5c0bc2b89385",
   "metadata": {},
   "source": [
    "### Creating benchmark datasets\n",
    "\n",
    "The section below creates the LLMs results dataframes and LLM benchmark resutls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71fe3822-b3f4-4713-aee7-07e9f8814d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_dataset(file_path:str) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Creates a data frame that is the product of running multiple\n",
    "    prompts through all the models and returns the response\n",
    "    message, model response time, and output tokens.\n",
    "\n",
    "    Inputs:\n",
    "    file_path (str): csv file path containing a singular column of prompts\n",
    "\n",
    "    Outputs:\n",
    "    dataset (pd.DataFrame): Output data frame of all prompts apply\n",
    "                            to the function of calling all LLM models.\n",
    "\n",
    "    \"\"\"\n",
    "    df_prompts = pd.read_csv(file_path, sep=\",\")\n",
    "    lst_prompts = df_prompts['questions'].values\n",
    "\n",
    "    lst_responses = []\n",
    "    counter = 0\n",
    "    print(\"Starting LLM API calls\")\n",
    "    for prompts in lst_prompts:\n",
    "        full_llm_res = llm_invoke_all(prompts)\n",
    "        lst_responses.append(full_llm_res)\n",
    "        counter += 1\n",
    "        print(\"Completed {} rounds\".format(counter))\n",
    "\n",
    "    lst_pd = []\n",
    "    for sublist in lst_responses:\n",
    "        for item in sublist:\n",
    "            lst_pd.append(item)\n",
    "\n",
    "    df_results = pd.DataFrame(lst_pd)\n",
    "    return df_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57c2d7a6-d106-462d-8930-17fd2035dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stats_llm(llm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a data frame that is a merge of two groupby \n",
    "    data frames of response time and output token.\n",
    "\n",
    "    Inputs: \n",
    "    llm_df (pd.DataFrame): Data frame of LLM responses\n",
    "\n",
    "    Outputs:\n",
    "    stats_df (pd.DataFrame): Data frame of LLM aggregation stats.\n",
    "    \n",
    "    \"\"\"\n",
    "    time_df = llm_df.groupby(\"model_name\")['resp_time'].aggregate(time_min='min',time_max='max',time_mean='mean')\n",
    "    time_df = time_df.reset_index()\n",
    "    time_df = time_df.sort_values('time_mean', ascending=True).reset_index(0, drop=True)\n",
    "\n",
    "    tkn_df = llm_df.groupby(\"model_name\")['output_tokens'].aggregate(tkn_min=\"min\", tkn_max=\"max\", tkn_mean=\"mean\")\n",
    "    tkn_df = tkn_df.reset_index()\n",
    "    tkn_df = tkn_df.sort_values(\"tkn_mean\", ascending=True).reset_index(0, drop=True)\n",
    "\n",
    "    stats_df = time_df.merge(tkn_df, how=\"inner\", on=\"model_name\")\n",
    "\n",
    "    return stats_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b7503-340c-42e9-8405-456832890a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_save(llm_df: pd.DataFrame, stats_llm: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Saves two data frames to separate json files.   \n",
    "    \"\"\"\n",
    "    folder_path = \"./OutputLLM/\"\n",
    "    os.makedirs(folder_path, exist_ok = True)\n",
    "\n",
    "    llm_df.to_json(folder_path + \"LLM_Results.json\", orient = \"records\", compression = \"infer\")\n",
    "\n",
    "    stats_llm.to_json(folder_path + \"LLM_Stats.json\", orient = \"records\", compression = \"infer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ac11d-6de5-4695-9766-3ade89884ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./Data/questions.csv\"\n",
    "df_llm = create_llm_dataset(csv_path)\n",
    "llm_stats = create_stats_llm(df_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e42ed379-84c2-4290-b6ff-914106e3d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save(df_llm, llm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bee705-fc23-4c9c-91b8-332d9426e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark statistics for the LLM Models.\")\n",
    "llm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee294dc4-644d-48ff-bfb4-cf4c15230daa",
   "metadata": {},
   "source": [
    "### Gradio Interface\n",
    "\n",
    "This section will present the functions of the LLMs as an interface through gradio.\n",
    "Users will select which model to use which allows them to enter a prompt to return \n",
    "the result.\n",
    "\n",
    "The function that invokes all the LLM will present the fastest models from top to bottom\n",
    "as a quick visual as to which model is best for the particular prompt.\n",
    "\n",
    "The third gradio interface is a simple file upload to look at the results that were saved\n",
    "as a json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c9531cc8-a71a-421b-a1e0-7021eb295342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface_singular = gr.Interface(fn = llm_response_gradio,\n",
    "                     inputs =[gr.Radio(list(model_dict.keys()), label=\"Model Name\"),\n",
    "                              gr.Textbox(label=\"Enter your Prompt\")],\n",
    "                     outputs = gr.Textbox(label=\"LLM Output\"),\n",
    "                     title = \"Nvidia LLM Invoker\",\n",
    "                     description = \"Choose a model and enter a prompt to invoke a LLM model from Nvidia AI Foundational Models.\")\n",
    "\n",
    "iface_singular.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "39834c71-8800-41ab-85ae-a89db93a93fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface_singular.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "00a336c3-28f8-4008-86a0-2146f34f6f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface_llm = gr.Interface(fn = llm_response_gradio_all,\n",
    "                         inputs = gr.Textbox(label=\"Enter your Prompt\"),\n",
    "                         outputs = gr.Textbox(label=\"Best Performing LLMs with respect to time.\"),\n",
    "                         title = \"Nvidia Multi-Model Invoker\",\n",
    "                         description = \"Enter a prompt to invoke multiple LLMs from Nvidia AI Foundational Models and return the response times.\"\n",
    ")\n",
    "\n",
    "iface_llm.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5c9a9b93-1613-4cfd-aac7-398e554f44f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface_llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d1e7f67f-dde3-494b-b204-192647339327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/gradio/route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/gradio/blocks.py\", line 1591, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/kz/z67jbs351w735mpt3w0l1b8r0000gn/T/ipykernel_27417/269447191.py\", line 2, in read_file\n",
      "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 310, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    }
   ],
   "source": [
    "def read_file(file):\n",
    "  with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    return f.read()\n",
    "\n",
    "interface = gr.Interface(fn=read_file, inputs=\"file\", outputs=\"text\")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0da8168a-f647-4f70-a351-0e602645e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "interface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9d065923-e20e-44be-aeb2-1527eac7d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                  Version\n",
      "---------------------------------------- ---------------\n",
      "accelerate                               0.26.1\n",
      "aiofiles                                 23.2.1\n",
      "aiohttp                                  3.9.3\n",
      "aiosignal                                1.3.1\n",
      "altair                                   5.2.0\n",
      "annotated-types                          0.6.0\n",
      "anyio                                    4.2.0\n",
      "appnope                                  0.1.3\n",
      "argon2-cffi                              23.1.0\n",
      "argon2-cffi-bindings                     21.2.0\n",
      "arrow                                    1.3.0\n",
      "asgiref                                  3.7.2\n",
      "asttokens                                2.4.1\n",
      "async-lru                                2.0.4\n",
      "asyncio                                  3.4.3\n",
      "asyncio-throttle                         1.0.2\n",
      "attrs                                    23.2.0\n",
      "Babel                                    2.14.0\n",
      "backoff                                  2.2.1\n",
      "bcrypt                                   4.1.2\n",
      "beautifulsoup4                           4.12.3\n",
      "bleach                                   6.1.0\n",
      "bs4                                      0.0.2\n",
      "build                                    1.0.3\n",
      "cachetools                               5.3.2\n",
      "certifi                                  2023.11.17\n",
      "cffi                                     1.16.0\n",
      "charset-normalizer                       3.3.2\n",
      "chroma-hnswlib                           0.7.3\n",
      "chromadb                                 0.4.22\n",
      "click                                    8.1.7\n",
      "colorama                                 0.4.6\n",
      "coloredlogs                              15.0.1\n",
      "comm                                     0.2.1\n",
      "contourpy                                1.2.0\n",
      "cryptography                             42.0.2\n",
      "cycler                                   0.12.1\n",
      "dataclasses-json                         0.6.4\n",
      "datasets                                 2.16.1\n",
      "debugpy                                  1.8.0\n",
      "decorator                                5.1.1\n",
      "defusedxml                               0.7.1\n",
      "Deprecated                               1.2.14\n",
      "dill                                     0.3.7\n",
      "distro                                   1.9.0\n",
      "docker                                   7.0.0\n",
      "executing                                2.0.1\n",
      "fastapi                                  0.109.0\n",
      "fastjsonschema                           2.19.1\n",
      "ffmpy                                    0.3.1\n",
      "filelock                                 3.13.1\n",
      "FlagEmbedding                            1.2.3\n",
      "flatbuffers                              23.5.26\n",
      "fonttools                                4.48.1\n",
      "fqdn                                     1.5.1\n",
      "frozenlist                               1.4.1\n",
      "fsspec                                   2023.10.0\n",
      "google-auth                              2.27.0\n",
      "googleapis-common-protos                 1.62.0\n",
      "gradio                                   4.18.0\n",
      "gradio_client                            0.10.0\n",
      "greenlet                                 3.0.3\n",
      "grpcio                                   1.60.1\n",
      "h11                                      0.14.0\n",
      "httpcore                                 1.0.2\n",
      "httptools                                0.6.1\n",
      "httpx                                    0.26.0\n",
      "huggingface-hub                          0.20.3\n",
      "humanfriendly                            10.0\n",
      "idna                                     3.6\n",
      "importlib-metadata                       6.11.0\n",
      "importlib-resources                      6.1.1\n",
      "ipykernel                                6.29.0\n",
      "ipython                                  8.21.0\n",
      "ipywidgets                               8.1.1\n",
      "isoduration                              20.11.0\n",
      "jedi                                     0.19.1\n",
      "Jinja2                                   3.1.3\n",
      "joblib                                   1.3.2\n",
      "json5                                    0.9.14\n",
      "jsonpatch                                1.33\n",
      "jsonpointer                              2.4\n",
      "jsonschema                               4.21.1\n",
      "jsonschema-specifications                2023.12.1\n",
      "jupyter                                  1.0.0\n",
      "jupyter_client                           8.6.0\n",
      "jupyter-console                          6.6.3\n",
      "jupyter_core                             5.7.1\n",
      "jupyter-events                           0.9.0\n",
      "jupyter-lsp                              2.2.2\n",
      "jupyter_server                           2.12.5\n",
      "jupyter_server_terminals                 0.5.2\n",
      "jupyterlab                               4.0.12\n",
      "jupyterlab_pygments                      0.3.0\n",
      "jupyterlab_server                        2.25.2\n",
      "jupyterlab-widgets                       3.0.9\n",
      "kiwisolver                               1.4.5\n",
      "kubernetes                               29.0.0\n",
      "langchain                                0.1.5\n",
      "langchain-community                      0.0.17\n",
      "langchain-core                           0.1.18\n",
      "langchain-openai                         0.0.5\n",
      "langchainhub                             0.1.14\n",
      "langsmith                                0.0.85\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               2.1.4\n",
      "marshmallow                              3.20.2\n",
      "matplotlib                               3.8.2\n",
      "matplotlib-inline                        0.1.6\n",
      "mdurl                                    0.1.2\n",
      "mistune                                  3.0.2\n",
      "mmh3                                     4.1.0\n",
      "monotonic                                1.6\n",
      "mpmath                                   1.3.0\n",
      "multidict                                6.0.4\n",
      "multiprocess                             0.70.15\n",
      "mypy-extensions                          1.0.0\n",
      "nbclient                                 0.9.0\n",
      "nbconvert                                7.14.2\n",
      "nbformat                                 5.9.2\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.2.1\n",
      "nltk                                     3.8.1\n",
      "notebook                                 7.0.7\n",
      "notebook_shim                            0.2.3\n",
      "numpy                                    1.26.3\n",
      "oauthlib                                 3.2.2\n",
      "onnxruntime                              1.17.0\n",
      "openai                                   1.10.0\n",
      "opentelemetry-api                        1.22.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.22.0\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.22.0\n",
      "opentelemetry-instrumentation            0.43b0\n",
      "opentelemetry-instrumentation-asgi       0.43b0\n",
      "opentelemetry-instrumentation-fastapi    0.43b0\n",
      "opentelemetry-proto                      1.22.0\n",
      "opentelemetry-sdk                        1.22.0\n",
      "opentelemetry-semantic-conventions       0.43b0\n",
      "opentelemetry-util-http                  0.43b0\n",
      "orjson                                   3.9.13\n",
      "overrides                                7.7.0\n",
      "packaging                                23.2\n",
      "pandas                                   2.2.0\n",
      "pandocfilters                            1.5.1\n",
      "parso                                    0.8.3\n",
      "pexpect                                  4.9.0\n",
      "pillow                                   10.2.0\n",
      "pip                                      23.3.2\n",
      "platformdirs                             4.2.0\n",
      "posthog                                  3.3.4\n",
      "prometheus-client                        0.19.0\n",
      "prompt-toolkit                           3.0.43\n",
      "protobuf                                 4.25.2\n",
      "psutil                                   5.9.8\n",
      "ptyprocess                               0.7.0\n",
      "pulsar-client                            3.4.0\n",
      "pure-eval                                0.2.2\n",
      "pyarrow                                  15.0.0\n",
      "pyarrow-hotfix                           0.6\n",
      "pyasn1                                   0.5.1\n",
      "pyasn1-modules                           0.3.0\n",
      "pycparser                                2.21\n",
      "pydantic                                 2.6.0\n",
      "pydantic_core                            2.16.1\n",
      "pydub                                    0.25.1\n",
      "Pygments                                 2.17.2\n",
      "pyparsing                                3.1.1\n",
      "PyPika                                   0.48.9\n",
      "pyproject_hooks                          1.0.0\n",
      "python-dateutil                          2.8.2\n",
      "python-dotenv                            1.0.1\n",
      "python-json-logger                       2.0.7\n",
      "python-multipart                         0.0.9\n",
      "pytz                                     2023.4\n",
      "pyvespa                                  0.39.0\n",
      "PyYAML                                   6.0.1\n",
      "pyzmq                                    25.1.2\n",
      "qtconsole                                5.5.1\n",
      "QtPy                                     2.4.1\n",
      "referencing                              0.33.0\n",
      "regex                                    2023.12.25\n",
      "requests                                 2.31.0\n",
      "requests-oauthlib                        1.3.1\n",
      "rfc3339-validator                        0.1.4\n",
      "rfc3986-validator                        0.1.1\n",
      "rich                                     13.7.0\n",
      "rpds-py                                  0.17.1\n",
      "rsa                                      4.9\n",
      "ruff                                     0.2.1\n",
      "safetensors                              0.4.2\n",
      "scikit-learn                             1.4.0\n",
      "scipy                                    1.12.0\n",
      "semantic-version                         2.10.0\n",
      "Send2Trash                               1.8.2\n",
      "sentence-transformers                    2.3.1\n",
      "sentencepiece                            0.1.99\n",
      "setuptools                               69.0.3\n",
      "shellingham                              1.5.4\n",
      "six                                      1.16.0\n",
      "sniffio                                  1.3.0\n",
      "soupsieve                                2.5\n",
      "SQLAlchemy                               2.0.25\n",
      "stack-data                               0.6.3\n",
      "starlette                                0.35.1\n",
      "sympy                                    1.12\n",
      "tenacity                                 8.2.3\n",
      "terminado                                0.18.0\n",
      "threadpoolctl                            3.2.0\n",
      "tiktoken                                 0.5.2\n",
      "tinycss2                                 1.2.1\n",
      "tokenizers                               0.15.1\n",
      "tomlkit                                  0.12.0\n",
      "toolz                                    0.12.1\n",
      "torch                                    2.2.0\n",
      "tornado                                  6.4\n",
      "tqdm                                     4.66.1\n",
      "traitlets                                5.14.1\n",
      "transformers                             4.37.2\n",
      "typer                                    0.9.0\n",
      "types-python-dateutil                    2.8.19.20240106\n",
      "types-requests                           2.31.0.20240125\n",
      "typing_extensions                        4.9.0\n",
      "typing-inspect                           0.9.0\n",
      "tzdata                                   2023.4\n",
      "uri-template                             1.3.0\n",
      "urllib3                                  2.2.0\n",
      "uvicorn                                  0.27.0.post1\n",
      "uvloop                                   0.19.0\n",
      "watchfiles                               0.21.0\n",
      "wcwidth                                  0.2.13\n",
      "webcolors                                1.13\n",
      "webencodings                             0.5.1\n",
      "websocket-client                         1.7.0\n",
      "websockets                               11.0.3\n",
      "wheel                                    0.42.0\n",
      "widgetsnbextension                       4.0.9\n",
      "wrapt                                    1.16.0\n",
      "xxhash                                   3.4.1\n",
      "yarl                                     1.9.4\n",
      "zipp                                     3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0c654-6f05-4f0e-a05f-aa9a7fff18be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
